\documentclass[10pt]{amsart}
\usepackage{fullpage,hyperref,vipul, graphicx}
\title{Review sheet for midterm 2}
\author{Math 152, Section 55 (Vipul Naik)}

\begin{document}
\maketitle

{\em I know, this review sheet is really really long! We really have
covered a lot of material and it was hard to trim the review sheet.}
The good news is that most of it should (hopefully!) sound pretty
familiar to you and hence you don't need to spend too much time on the
majority of stuff. But {\em do} take note of stuff that doesn't seem
immediatel familiar to you.

The document is arranged as follows. The initial sections/subsections
correspond to topics. Each subsection has two sets of points,
``Words'' which includes basic theory and definitions, and ``Actions''
which provides information on strategies for specific problem
types. In some cases, there are additional points. The lists of points
are largely the same as the executive summaries at the beginning of
the lecture notes, though some additional points (that were omitted
from the original executive summaries) have been added.

For most subsections (except the last few, that deal with integration
techniques), there is also an ``Error-spotting exercises'' list. We
will be doing these exercises in the review session, though you may
benefit by trying them out in advance.

The sections titled ``Graphing and miscellanea on functions'' (Section
5) and ``Tricky topics'' (Section 6) cover a bunch of topics and
question types that habitually confuse students.

The section titled ``High yield practice'' lists (without details) the
areas where I think practice is most helpful if you feel you're
already fairly thorough with the basic formulas. If you feel you are
on top of AP-level material, for instance, then these are the areas
where most of your energies should be devoted.

The end of the document has some ``Quickly'' lists. These are lists of
things you should be able to accomplish quickly. This includes
numerical values, formulas, graphs, examples, and counterexamples,
that should be ready for immediate recall in the test environment. I
simply provide a list and do not include details of all the formulas
and graphs.



{\em This document does not re-review material already covered in the
review sheet for midterm 1. It is your responsibility to go through
that review sheet again and make sure you have mastered all the
material there.} We could cover in the review session some topics
there that you are having difficulty with, but that will not be a
priority.

\section{Left-overs from differentiation basics}

\subsection{Derivative as rate of change}

Words...

\begin{enumerate}
\item The derivative of $A$ with respect to $B$ is the rate of change
  of $A$ with respect to $B$. Thus, to determine rates of change of
  various quantities, we can use the techniques of differentiation.
\item If there are three linked quantities that are changing together
  (e.g., different measures for a circle such as radius, diameter,
  circumference, area) then we can use the chain rule.
\end{enumerate}

Most of the actions in this case are not more than a direct
application of the words.

\subsection{Implicit differentiation}

Words...

\begin{enumerate}
\item Suppose there is a curve in the plane, whose equation cannot be
  manipulated easily to write one variable in terms of the other. We
  can use the technique of implicit differentiation to determine the
  derivative, and hence the slope of the tangent line, at different
  points to the curve.
\item For a curve where neither variable is expressible as a function
  of the other, the notion of derivative still makes sense as long as
  {\em locally}, we can get $y$ as a function of $x$. For instance,
  for the circle $x^2 + y^2 = 1$, $y$ is not a function of $x$, but if
  we restrict attention to the part of the circle above the $x$-axis,
  then on this restricted region, $y$ is a function of $x$.
\item In some cases, even when one variable is expressible as a
  function of the other, implicit differentiation is easier to handle
  as it may involve fewer messy squareroot symbols.
\end{enumerate}

Actions ...

\begin{enumerate}
\item To determine the derivative using implicit differentiation,
  write down the equations of both curves, differentiate both sides
  with respect to $x$, and simplify using all the differentiation
  rules, to get everything in terms of $x$, $y$, and $dy/dx$. Isolate
  the $dy/dx$ term in terms of $x$ and $y$, and compute it at whatever
  point is needed.
\item This procedure can be iterated to compute higher order
  derivatives at specific points on the curve where the curve locally
  looks like a function.
\end{enumerate}

\section{Increase/decrease, maxima/minima, concavity, inflection, tangents, cusps, asymptotes}

\subsection{Rolle's, mean value, increase/decrease, maxima/minima}

Words...

\begin{enumerate}
\item If a function $f$ is continuous on the closed interval $[a,b]$
  and differentiable on the open interval $(a,b)$, and $f(a) = f(b) =
  0$, then there exists $c \in (a,b)$ such that $f'(c) = 0$. This is
  called {\em Rolle's theorem} and is a consequence of the
  extreme-value theorem.
\item If a function $f$ is continuous on the closed interval $[a,b]$
  and differentiable on the open interval $(a,b)$, then there exists
  $c \in (a,b)$ such that $f'(c)$ is the difference quotient $(f(b) -
  f(a))/(b - a)$. This result is called the {\em mean-value
  theorem}. Geometrically, it says that for any chord, there is a
  parallel tangent. Another way of thinking about it is that every
  difference quotient is equal to a derivative at some intermediate
  point.
\item If $f$ is a function and $c$ is a point such that $f(c) \ge
  f(x)$ for $x$ to the immediate left of $c$, we say that $c$ is a
  local maximum from the left. In this case, the left-hand derivative
  of $f$ at $c$, if it exists, is greater than or equal to zero. This
  is because the difference quotient is greater than or equal to
  zero. Local maximum from the right implies that the right-hand
  derivative (if it exists) is $\le 0$, local minimum from the left
  implies that the left-hand derivative (if it exists) is $\le 0$, and
  local minimum from the right implies that the right-hand derivative
  (if it exists) is $\ge 0$. Even in the case of {\em strict} local
  maxima and minima, we still need to retain the equality sign on the
  derivative because it occurs as a {\em limit} and a limit of
  positive numbers can still be zero.
\item If $c$ is a point where $f$ attains a local maximum (i.e., $f(c)
  \ge f(x)$ for all $x$ close enough to $c$ on both sides), then
  $f'(c)$, if it exists, is equal to zero. Similarly for local
  minimum.
\item A {\em critical point} for a function is a point where either
  the function is not differentiable or the derivative is zero. All
  local maxima and local minima must occur at critical points.
\item If $f'(x) > 0$ for all $x$ in the open interval $(a,b)$, $f$ is
  increasing on $(a,b)$. Further, if $f$ is one-sided continuous at
  the endpoint $a$ and/or the endpoint $b$, then $f$ is increasing on
  the interval including that endpoint. Similarly, $f'(x) < 0$ implies
  $f$ decreasing.
\item If $f'(x) > 0$ everywhere except possibly at some isolated
  points (so that they don't cluster around any point) where $f$ is
  still continuous, then $f$ is increasing everywhere.
\item If $f'(x) = 0$ on an open interval, $f$ is constant on that
  interval, and it takes the same constant value at an endpoint where
  it's continuous from the appropriate side.
\item If $f$ and $g$ are two functions that are both continuous on an
  interval $I$ and have the same derivative on the interior of $I$,
  then $f - g$ is a constant function.

  {\em Note: Due to an oversight, the remaining items in this list
  were not included in the executive summary to the lecture notes.}

\item There is a {\em first derivative test} which provides a
  sufficient (though not necessary) condition for a local extreme
  value: it says that if the first derivative is nonnegative
  (respectively positive) on the immediate left of a critical point,
  that gives a strict local maximum (respectively local maximum) from
  the left. If the first derivative is negative on the immediate left,
  we get a strict local minimum from the left. If the first derivative
  is positive on the immediate right, we get a strict local minimum
  from the right, and if it is negative on the immediate right, we get
  a strict local maximum from the right.

  The first derivative test is similar to the corresponding
  ``one-sided derivative'' test, but is somewhat stronger for a
  variety of situations because in many cases, one-sided derivatives
  are zero, which is inconclusive, whereas the first derivative test
  fails us more rarely.
\item The second derivative test states that if $f$ has a critical
  point $c$ where it is twice differentiable, then $f''(c) > 0$
  implies that $f$ has a local minimum at $c$, and $f''(c) < 0$
  implies that $f$ has a local maximum at $c$.
\item There are also higher derivative tests that work for critical
  points $c$ where $f'(c) = 0$. These work as follows: we look for the
  smallest $k$ such that $f^{(k)}(c) \ne 0$. If this $k$ is even, then
  $f$ has a local extreme value at $c$, and the nature (max versus
  min) depends on the sign of $f^{(k)}(c)$ (max if negative, min if
  positive). If $k$ is odd, then we have what we'll see soon is a
  point of inflection.
\item To determine absolute maxima/minima, the candidates are: points
  of discontinuity, boundary points of domain (whether included in
  domain or outside the domain; if the latter, then limiting),
  critical points (derivative zero or undefined), and limiting cases
  at $\pm \infty$.
\end{enumerate}

Actions... (think of examples that you've done)

\begin{enumerate}
\item Rolle's theorem, along with the more sophisticated formulations
  involving increasing/decreasing, tell us that there is an intimate
  relationship between the zeros of a function and the zeros of its
  derivative. Specifically, between any two zeros of the function,
  there is a zero of its derivative. Thus, if a function has $r$
  zeros, the derivative has at least $r - 1$ zeros, with at least one
  zero between any two consecutive zeros of $f$.
\item The more sophisticated version tells us that between any two
  zeros of a differentiable function, the function must attain a local
  maximum or local minimum. So, if the function is increasing
  everywhere or decreasing everywhere, there is at most one zero.
\item The mean-value theorem allows us to use bounds on the derivative
  of a function to bound the overall variation, or change, in the
  function. This is because if the derivative cannot exceed some
  value, then the difference quotient also cannot exceed that value,
  which means that the function cannot change too quickly on average.
\item To determine regions where a function is increasing and
  decreasing, we find the derivative and determine regions where the
  derivative is positive, zero, and negative.
\item To determine all the local maxima and local minima of a
  function, find all the critical points. To find the critical points,
  solve $f' = 0$ and also consider, as possible candidates, all the
  points where the function changes definition. {\em Although a point
  where the function changes definition need not be a critical point,
  it is a very likely candidate.}
\item {\em This item was missed in the original executive summary}: To
  determine absolute maxima and absolute minima, find all candidates
  (discontinuity, endpoints, limiting cases, boundary points),
  evaluate at each, and compare. Note that any absolute maximum must
  arise as a local or endpoint maximum. However, instead of first
  determining which critical points give local maxima by the
  derivative tests, we can straightaway compute values everywhere and
  compare, if our interest is solely in finding the absolute maximum
  and minimum.
\end{enumerate}

Error-spotting exercises

\begin{enumerate}
\item If a function $f$ has a local maximum at a point $c$ in its
  domain, then $f$ is increasing on the immediate left of $c$ and
  decreasing on the immediate right of $c$.
\item Consider the function:

  $$f(x) := \lbrace\begin{array}{rl} x^3 - 12x + 14, & x \le 1 \\ x^2 - 6x + 8, & x > 1 \\\end{array}$$

  The derivative is:

  $$f'(x) = \lbrace\begin{array}{rl} 3x^2 - 12, & x \le 1 \\ 2x - 6, & x > 1 \\\end{array}$$

  The solutions for $f'(x) = 0$ are $x = -2$ and $x = 2$ (for the $x
  \le 1$ case) and $x = 3$ (the $x > 1$ case). Thus, the critical
  points are at $x = -2$, $x = 2$, and $x = 3$.
\item Consider the function:

  $$f(x) := x^4 - x + 1$$

  The derivative is:

  $$f'(x) = 4x^3 - 1$$

  Solve $f'(x) = 0$ and we get $x = (1/4)^{1/3}$. Thus, $f$ has a
  local maximum at $x = (1/4)^{1/3}$. The local maximum value is:

  $$4((1/4)^{1/3})^3 - 1$$

  which is $0$.

\item Consider the function

  $$f(x) := \frac{1}{x^3 - 1}$$

  The derivative is:

  $$f'(x) = \frac{3x^2}{(x^3 - 1)^2}$$

  The derivative is zero at $x = 0$, so that gives a critical
  point. Also, the derivative is undefined at $x = 1$, so that gives
  another critical point for $f$.

\item An everywhere differentiable function $f$ on $\R$ has critical
  points at $2$, $5$, and $9$ with corresponding function values $11$,
  $16$, and $3$ respectively. Thus, the absolute maximum value of $f$
  is $16$ and the absolute minimum value is $3$.
\end{enumerate}
 
\subsection{Concave up/down and points of inflection}

Words ...

\begin{enumerate}
\item A function is called {\em concave up} on an interval if it is
  continuous and its first derivative is continuous and increasing on
  the interval. If the function is twice differentiable, this is
  equivalent to requiring that the second derivative be positive
  except possibly at isolated points, where it can be zero. (Think
  $x^4$, whose first derivative, $4x^3$, is increasing, and the second
  derivative is positive everywhere except at $0$, where it is zero).
\item A function is called {\em concave down} on an interval if it is
  continuous and its first derivative is continuous and decreasing on
  the interval. If the function is twice differentiable, this is
  equivalent to requiring that the second derivative be negative
  except possibly at isolated points, where it can be zero.
\item A {\em point of inflection} is a point where the sense of
  concavity of the function changes. A point of inflection for a
  function is a point of local extremum for the first derivative.
\item Geometrically, at a point of inflection, the tangent line to the
  graph of the function {\em cuts through} the graph.
\end{enumerate}

Actions ...

\begin{enumerate}

\item To determine points of inflection, we first find critical points
  for the first derivative (which are points where this derivative is
  zero or undefined) and then use the first or second derivative test
  at these points. Note that these derivative tests are applied to
  the first derivative, so the first derivative here is the second
  derivative and the second derivative here is the third derivative.
\item In particular, if the second derivative is zero and the third
  derivative exists and is nonzero, we have a point of inflection.
\item A point where the first two derivatives are zero could be a
  point of local extremum or a point of inflection. To find out which
  one it is, we either use sign changes of the derivatives, or we use
  higher derivatives.
\item Most importantly, the second derivative being zero does {\em
  not} automatically imply that we have a point of inflection.
\item If the third derivative is zero, we can use a higher derivative
  test again. The upshot is that if the first $k \ge 2$ for which
  $f^{(k)}(c) \ne 0$ is even, then we do not have a point of
  inflection, but if the first $k$ is odd, then we have a point of
  inflection.
\end{enumerate}

Error-spotting exercises ...

\begin{enumerate}
\item Consider the function

  $$f(x) := 3x^5 - 5x^4 + 12x + 17$$

  The derivative is:

  $$f'(x) = 15x^4 - 20x^3 + 12$$

  The second derivative is:

  $$f''(x) = 60x^3 - 60x^2$$

  The zeros of this are $x = 0$ and $x = 1$. The function thus has
  points of inflection at the points on the graph corresponding to $x
  = 0$ and $x = 1$.
\item To check whether a critical point for the first derivative gives
  a point of inflection for the graph of the function, we need to
  check the sign of the third derivative. If the third derivative is
  nonzero, we get a point of inflection. If the third derivative is
  zero, then we {\em do not} get a point of inflection.
\end{enumerate}

\subsection{Tangents, cusps, and asymptotes}

Words...

\begin{enumerate}

\item We say that $f$ has a horizontal asymptote with value $L$ if
  $\lim_{x \to \infty} f(x) = L$ or $\lim_{x \to -\infty} f(x) =
  L$. Sometimes, both might occur. (In fact, in almost all the
  examples you have seen, the limits at $\pm \infty$, if finite, are
  both equal).
\item We say that $f$ has a vertical asymptote at $c$ if $\lim_{x \to
  c^-} f(x) = \pm \infty$ and/or $\lim_{x \to c^+} f(x) = \pm
  \infty$. Note that in this case, it usually also happens that $f'(x)
  \to \pm \infty$ on the relevant side, with the sign the same as that
  of $f(x)$'s approach if the approach is from the left and opposite
  to that of $f(x)$'s approach if the approach is from the
  right. However, this is not a foregone conclusion.
\item We say that $f$ has a vertical tangent at the point $c$ if $f$
  is continuous (and finite) at $c$ and $\lim_{x \to c} f'(x) = \pm
  \infty$, with the {\em same sign of infinity} from both sides. If
  $f$ is increasing, this sign is $+\infty$, and if $f$ is decreasing,
  this sign is $-\infty$. Geometrically, points of vertical tangent
  behave a lot likepoints of inflection and usually {\em do} give
  points of inflection (in the sense that the tangent line cuts
  through the graph). Think $x^{1/3}$.
\item We say that $f$ has a vertical cusp at the point $c$ if $f$ is
  continuous (and finite) at $c$ and $\lim_{x \to c^-} f'(x)$ and
  $\lim_{x \to c^+} f'(x)$ are infinities of opposite sign. In other
  words, $f$ takes a sharp about-turn at the $x$-value of $c$. Think
  $x^{2/3}$.
\item We say that $f$ is asymptotic to $g$ if $\lim_{x \to \infty}
  f(x) - g(x) = \lim_{x \to -\infty} f(x) - g(x) = 0$. In other words,
  the graphs of $f$ and $g$ come progressively closer as $|x|$ becomes
  larger. (We can also talk of one-sided asymptoticity, i.e.,
  asymptotic only in the positive direction or only in the negative
  direction). When $g$ is a {\em nonconstant linear function}, we say
  that $f$ has an {\em oblique asymptote}. Horizontal asymptotes are a
  special case, where one of the functions is a constant function.
\end{enumerate}

Actions...

\begin{enumerate}
\item Finding the horizontal asymptotes involves computing limits as
  the domain value goes to infinity. Finding the vertical asymptotes
  involves locating points in the domain, or the boundary of the
  domain, where the function limits off to infinity. For both of
  these, it is useful to remember the various rules for limits related
  to infinities.
\item Remember that for a vertical tangent or vertical cusp at a
  point, it is necessary that the function be continuous (and take a
  finite value). So, we not only need to find the points where the
  derivative goes off to infinity, we also need to make sure those are
  points where the function is continuous. Thus, for the function
  $f(x) = 1/x$, $f'(x) \to - \infty$ on both sides as $x \to 0$, but
  we do {\em not} obtain a vertical tangent -- rather, we obtain a
  vertical asymptote.
\end{enumerate}

Cute fact: Rational functions are asymptotically polynomial, and the
polynomial to which a given rational function is asymptotic (both
directions) is obtained by doing long division and looking at the
quotient. If the degree of the numerator is one more than that of the
denominator, we get an oblique (linear) asymptote. If the numerator
and denominator have equal degree, we get a horizontal asymptote (both
directions) with nonzero value. If the numerator has smaller degree,
the $x$-axis is the horizontal asymptote (both directions).

Error-spotting exercises

\begin{enumerate}
\item If $\lim_{x \to \infty} f(x) = L$ with $L$ a finite number, then
  $\lim_{x \to \infty} f'(x) = 0$.
\item If $\lim_{x \to \infty} f'(x) = 0$, then $\lim_{x \to \infty}
  f(x) = L$, with $L$ a finite number.
\item If $f'$ has a vertical tangent at a point $a$ in its domain,
  then $f$ has a point of inflection at $(a,f(a))$.
\item If $f'$ has a vertical cusp at a point $a$ in its domain, then
  $f$ has a local extreme value at $a$.
\item Suppose $f$ and $g$ are functions defined on all of
  $\R$. Suppose $f$ has a vertical tangent at a point $a$ in its
  domain and $g$ has a vertical tangent at a point $b$ in its
  domain. Then $f + g$ has a vertical tangent at $a + b$ and $f - g$
  has a vertical tangent at $a - b$.
\item Suppose $f$ and $g$ are functions, both defined on $\R$. Suppose
  $f$ and $g$ both have vertical tangents at a point $a$ in their
  domain (i.e., at the same point in the domain). Then, the sum $f +
  g$ also has a vertical tangent at $a$.
\item Suppose $f$ and $g$ are functions, both defined on $\R$. Suppose
  $f$ and $g$ both have vertical tangents at a point $a$ in their domain (i.e.,
  at the same point in the domain). Then, the pointwise product $f
  \cdot g$ also has a vertical tangent at $a$. {\em This is trickier
  than it looks!}
\end{enumerate}
\section{Max-min problems}

Words...

\begin{enumerate}
\item In real-world situations, maximization and minimization problems
  typically involve multiple variables, multiple constraints on those
  variables, and some objective function that needs to be maximized or
  minimized.
\item The only thing we know to solve such problems is to reduce
  everything in terms of one variable. This is typically done by {\em
  using up} some of the constraints to express the other variables in
  terms of that variable.
\item The problem then typically boils down to a
  maximization/minimization problem of a function in a single variable
  over an interval. We use the usual techniques for understanding this
  function, determining the local extreme values, determining the
  endpoint extreme values, and determining the absolute extreme
  values.
\end{enumerate}

{\em Special note: For integer optimization, please refer back to the
lecture notes!}

Actions... (think of examples; also review the notes on max-min problems)

\begin{enumerate}
\item Extremes sometimes occur at endpoints but these endpoints could
  correspond to degenerate cases. For instance, of all the rectangles
  with given perimeter, the square has the maximum area, and the
  minimum occurs in the degenerate case of a rectangle where one side
  has length zero.
\item Some constraints on the variables we have are explicitly stated,
  while others are implicit. Implicit constraints include such things
  as nonnegativity constraints. {\em Some of these implicit
  constraints may be on variables other than the single variable in
  terms of which we eventually write everything.}
\item After we have obtained the objective function in terms of one
  variable, we are in a position to throw out the other
  variables. However, before doing so, it is {\em necessary to
  translate all the constraints into constraints on the one variable
  that we now have}. 
\item When our intent is to maximize a function, it is sometimes
  useful to maximize an equivalent function that is easier to
  visualize or differentiate. For instance, to maximize $\sqrt{f(x)}$
  is equivalent to maximizing $f(x)$ if $f(x)$ is nonnegative. With
  this way of thinking about equivalent functions, we can make sure
  that the actual function that we differentiate is easy to
  differentiate. The main criterion is that the two functions should
  rise and fall together. (Analogous observations apply for
  minimizing) Remember, however, that to calculate the {\em value} of
  the maximum/minimum, you should go back to the original function.
\item Sometimes, there are other parameters in the
  maximization/minimization problem that are {\em unknown constants},
  and the final solution is expected to be in terms of those
  constants. In rare cases, the nature of the function, and hence the
  nature of maxima and minima, depends on whether those constants fall
  in particular intervals. {\em If you find this to be the case, go
  back to the original problem and see whether the real-world
  situation it came from constrains the constants to one of the
  intervals}.
\item For some geometrical problems, the maximization/minimization can
  be done trigonometrically. Here, we make a clever choice of an angle
  that controls the {\em shape} of the figure and then use the
  trigonometric functions of that angle. This could provide alternate
  insight into maximization.
\end{enumerate}

Smart thoughts for smart people ...

\begin{enumerate}
\item Before getting started on the messy differentiation to find
  critical points, think about the constraints and the endpoints. Is
  it obvious that the function will attain a minimum/maximum at one of
  the endpoints? What are the values of the function at the endpoints?
  (If no endpoints, take limiting values as you go in one direction of
  the domain). Is there an intuitive reason to believe that the
  function attains its optimal value somewhere {\em in between} rather
  than at an endpoint? Is there some kind of trade-off to be made? Are
  there some things that can be said qualitatively about where the
  trade-off is likely to occur?
\item Feel free to convert your function to an equivalent function
  such that the two functions rise and fall together. This reduces the
  burden of messy expressions.
\item {\em Cobb-Douglas production}: For $p,q > 0$, the function $x
  \mapsto x^p(1 - x)^q$ attains a local maximum at $p/(p + q)$. In
  fact, this is the absolute maximum on $[0,1]$, and the function
  value is $p^pq^q/(p + q)^{p + q}$. This is important because this function
  appears in disguise all the time (e.g., maximizing area of rectangle
  with given perimeter, etc.)
\item A useful idea is that when dividing a resource into two
  competing uses, and one use is hands-down better than the other, the
  {\em best} use happens when the entire resource is devoted to the
  better use. However, the {\em worst} may well happen somewhere in
  between, because divided resources often perform even worse than
  resources devoted wholeheartedly to a bad use. This is seen in
  perimeter allocation to boundaries with the objective function being
  the total area, and area allocation to surfaces with the objective
  function being the total volume.
\item When we want to {\em maximize} something subject to a collection
  of many constraints, the most relevant constraint is the {\em
  minimum} one. Think of the ladder-through-the-hallway problem, or
  the truck-going-under-bridges problem. 
\end{enumerate}

Error-spotting exercises

\begin{enumerate}
\item The absolute maximum among the values of a (?) function (of reals)
  at integers is attained at the integer closest to the point at which
  it attains its absolute maximum among all reals.
\item The absolute maximum among the values of a (?) function (of reals)
  at integers is attained at one of the integers closest to the point
  at which it attains a local maximum.
\item To maximize the sum of two functions is equivalent to maximizing
  each one separately and then finding the common point of maximum.
\item If $f$ is a function that is continuous and concave up on an
  interval $[a,b]$, then the absolute minimum of $f$ always occurs at
  an interior point and the absolute maximum of $f$ always occurs at
  an endpoint. {\em This is a little subtle, because it's almost but
  not completely correct. Think through it clearly!}
\item Consider the function:

  $$f(x) := \lbrace\begin{array}{rl} x^3, & 0 \le x \le 1 \\ x^2, & 1 < x \le 2 \\\end{array}$$

  Then, $f'$ is increasing on $[0,2]$, so $f$ is concave up on
  $[0,2]$.
\end{enumerate}

\section{Definite and indefinite integration}

\subsection{Definition and basics}

Words ...

\begin{enumerate}
\item The definite integral of a continuous (though somewhat weaker
  conditions also work) function $f$ on an interval $[a,b]$ is a measure
  of the signed area between the graph of $f$ and the $x$-axis. It
  measures the {\em total value} of the function.
\item For a partition $P$ of $[a,b]$, the lower sum $L_f(P)$ adds up,
  for each subinterval of the partition, the length of that interval
  times the minimum value of $f$ over that interval. The upper sum
  adds up, for each subinterval of the partition, the length of that
  interval times the maximum value of $f$ on that subinterval.
\item Every lower sum of $f$ is less than or equal to every upper sum
  of $f$.
\item The {\em norm} or {\em size} of a partition $P$, denoted $\| P
  \|$, is defined as the maximum of the lengths of its
  subintervals.
\item If $P_1$ is a finer partition than $P_2$, i.e., every interval
  of $P_1$ is contained in an interval of $P_2$, then the following
  three things are true: (a) $L_f(P_2) \le L_f(P_1)$, (b) $U_f(P_1)
  \le U_f(P_2)$, and (c) $\| P_1 \| \le \| P_2 \|$.
\item If $\lim_{\| P \| \to 0} L_f(P) = \lim_{\| P \| \to 0} U_f(P)$,
  then this common limit is termed the {\em integral} of $f$ on the
  interval $[a,b]$.
\item We can define $\int_a^b f(x) \, dx$ as above if $a < b$. If $a =
  b$ the integral is defined to be $0$. If $a > b$, the integral is
  defined as $-\int_b^a f(x) \, dx$.
\item A continuous function on $[a,b]$ has an integral on $[a,b]$. A
  piecewise continuous function where one-sided limits exist and are
  finite at every point is also integrable.
\end{enumerate}

Actions ...

\begin{enumerate}
\item For constant functions, the integral is just the product of the
  value of the function and the length of the interval.
\item Points don't matter. So, if we change the value of a function at
  one point while leaving the other values unaffected, the integral
  does not change.
\item A first-cut lower and upper bound on the integral can be
  obtained using the {\em trivial} partition, where we do not
  subdivide the interval at all. The upper bound is thus the maximum
  value times the length of the interval, and the lower bound is the
  minimum value times the length of the interval.
\item The finer the partition, the closer the lower and upper bounds,
  and the better the approximation we obtain for the integral.
\item A very useful kind of partition is a {\em regular partition},
  which is a partition where all the parts have the same length. If
  the integral exists, we can calculate the actual integral as
  $\lim_{n \to \infty}$ of the upper sums or the lower sums for a
  regular partition into $n$ parts.
\item When a function is increasing on some parts of the interval and
  decreasing on other parts, it is useful to choose the partition in
  such a way that on each piece of the partition, the function is
  either increasing throughout or decreasing throughout. This way, the
  maximum and minimum occur at the endpoints in each piece. In
  particular, try to choose all points of local extrema as points of
  partition.
\end{enumerate}

Error-spotting exercises ...

\begin{enumerate}
\item If $P_1$ and $P_2$ are partitions of $[a,b]$ and $\| P_2 \| \le
  \| P_1 \|$, then $P_2$ is finer than $P_1$.
\item If $P_1$ and $P_2$ are partitions of $[a,b]$ such that $P_2$ is
  finer than $P_1$, and $f$ is a bounded function on $[a,b]$, then
  $L_f(P_2) \le L_f(P_1)$ and $U_f(P_2) \le U_f(P_1)$.
\item For any continuous function $f$ on $[a,b]$, the number of parts
  $n$ we need in a regular partition of $[a,b]$ so that the integral
  is bounded in an interval of length $L$ is proportional to $1/n$.
\end{enumerate}
\subsection{Definite integral, antiderivative, and indefinite integral}

Words ..

\begin{enumerate}
\item We have $\int_a^b f(x) \, dx + \int_b^c f(x) \, dx = \int_a^c
  f(x) \, dx$.
\item We say that $F$ is an antiderivative for $f$ if $F' = f$.
\item For a continuous function $f$ defined on a closed interval
  $[a,b]$, and for a point $c \in [a,b]$, the function $F$ given by
  $F(x) = \int_c^x f(t) \, dt$ is an antiderivative for $f$.
\item If $f$ is continuous on $[a,b]$ and $F$ is a function continuous
  on $[a,b]$ such that $F' = f$ on $(a,b)$, then $\int_a^b f(x) \, dx
  = F(b) - F(a)$.
\item The two results above essentially state that differentiation and
  integration are opposite operations.
\item For a function $f$ on an interval $[a,b]$, if $F$ and $G$ are
  antiderivatives, then $F - G$ is constant on $[a,b]$. Conversely, if
  $F$ is an antiderivative of $f$, so is $F$ plus any constant.
\item The {\em indefinite integral} of a function $f$ is the
  collection of all antiderivatives for the function. This is
  typically written by writing one antiderivative plus $C$, where $C$
  is an arbitrary constant. We write $\int f(x) \, dx$ for the
  indefinite integral. Note that there are no upper and lower limits.
\item Both the definite and the indefinite integral are additive. In
  other words, $\int f(x) \, dx + \int g(x) \, dx = \int f(x) + g(x)
  \, dx$. The analogue holds for definite integrals, with limits.
\item We can also pull constants multiplicatively out of integrals.
\item Note the important formula:

  $$\frac{d}{dx} \int_{u(x)}^{v(x)} f(t) \, dt = f(v(x))v'(x) - f(u(x))u'(x)$$
\end{enumerate}

Actions ...

\begin{enumerate}
\item Consider the integration problem:

  $$\int \frac{2x}{x^3 + 1} \, dx = \int \frac{2x}{x^3} \, dx + \int \frac{2x}{1} \, dx = \int \frac{2}{x^2} \, dx + \int 2x \, dx = \frac{-2}{x} + x^2 + C$$
\item To do a definite integral, find any one antiderivative and
  evaluate it between limits.
\item An important caveat: when using antiderivatives to do a definite
  integral, it is important to make sure that the antiderivative is
  defined and continuous everywhere on the interval of
  integration. (Think of the $1/x^3$ example). 
\item To do an indefinite integral, find any antiderivative and put a
  $+ C$.
\item To find an antiderivative, use the additive splitting and
  pulling constants out, and the fact that $\int x^r \, dx = x^{r +
    1}/(r + 1)$.
\end{enumerate}

Error-spotting exercises ...

\begin{enumerate}
\item Consider the function $f(x) := \int_x^{x^2} \sin x \, dx$. Then
  $f'(x) = \sin(x^2) - \sin(x)$.
\item Suppose $f$ is a function on the nonzero reals such that $f'(x)
  = 1/x^2$ for all $x \in \R$. Then, we must have $f(x) = 1/x + C$ for
  some constant $C$.
\end{enumerate}

\subsection{Higher derivatives, multiple integrals, and initial/boundary conditions}

Actions ...

\begin{enumerate}
\item The simplest kind of {\em initial value problem} (a notion we
  will encounter again when we study differential equations) is as
  follows. The $k^{th}$ derivative of a function is given on the
  entire domain. Next, the {\em values} of the function and the first
  $k - 1$ derivatives are given at a single point of the domain. We
  can use this data to find the function. Step by step, we find
  derivatives of lower orders. First, we integrate the $k^{th}$
  derivative to get that the $(k-1)^{th}$ derivative is of the form
  $F(x) + C$, where $C$ is unknown. We now use the value of the
  $(k-1)^{th}$ derivative at the given point to find $C$. Now, we have
  the $(k-1)^{th}$ derivative. We proceed now to find the $(k-2)^{th}$
  derivative, and so on.
\item Sometimes, we may be interested in finding {\em all} functions
  with a given second derivative $f$. For this, we have to perform an
  indefinite integration twice. The net result will be a general
  expression of the form $F(x) + C_1x + C_2$, where $F$ is a function
  with $F'' = f$, and $C_1$ and $C_2$ are arbitrary constants. In
  other words, we now have {\em up to constants or linear functions}
  instead of {\em up to constants} as our degree of ambiguity.
\item More generally, if the $k^{th}$ derivative of a function is
  given, the function is uniquely determined up to additive
  differences of polynomials of degree strictly less than $k$ (so,
  degree at most $k - 1$). The number of free constants that can take
  arbitrary real values is $k$ (namely, the coefficients of the
  polynomial).
\item This general expression is useful if, instead of an initial
  value problem, we have a boundary value problem. Suppose we are
  given $G''$ as a function, and we are given the value of $G$ at two
  points. We can then first find the general expression for $G$ as $F
  + C_1x + C_2$. Next, we plug in the values to get a system of two
  linear equations, that we solve in order to determine $C_1$ and
  $C_2$, and hence $G$.
\end{enumerate}

Error-spotting exercises ...

\begin{enumerate}
\item Suppose $F$ and $G$ are everywhere $k$ times differentiable
  functions for $k$ a positive integer. If the $k^{th}$ derivatives of
  the functions $F$ and $G$ are equal, then $F - G$ is a polynomial of
  degree $k$.
\item Suppose $F$ is a function defined on nonzero reals and $F''(x) =
  1/x^3$ for all $x$. Then, $F$ is of the form $F(x) = 1/x + C$ where
  $C$ is a real constant.
\end{enumerate}
\subsection{Reversing the chain rule}

Words ...

\begin{enumerate}
\item The chain rule states that $(f \circ g)' = (f' \circ g) \cdot
  g'$.
\item Some integrations require us to reverse the chain rule. For
  this, we need to realize the integrand that we have in the form of the
  right-hand side of the chain rule.
\item The first step usually is to find the correct function $g$,
  which is the {\em inner function} of the composition, then to adjust
  constants suitably so that the remaining term is $g'$, and then
  figure out what $f'$ is. Finally, we find an antiderivative for
  $f'$, which we can call $f$, and then compute $f \circ g$.
\item A slight variant of this method (which is essentially the same)
  is the substitution method, where we identify $g$ just as before,
  try to spot $g'$ in the integrand as before, and then put $u = g(x)$
  and rewrite the integral in terms of $u$.
\end{enumerate}

Actions ...

\begin{enumerate}
\item Good targets for $u$ in the $u$-substitution are things that are
  shielded.
\item In general, things in denominators, with non-integer exponents,
  or with complicated composites appended to them are good targets for
  $u$.
\end{enumerate}

\subsection{$u$-substitutions for definite integrals}

Words ... (try to recall the numerical formulations)

\begin{enumerate}
\item When doing the $u$-substitution for definite integrals, we
  transform the upper and lower limits of integration by the
  $u$-function.
\item Note that the $u$-substitution is valid only when the
  $u$-function is well-defined on the entire interval of integration.
\item The integral of a translate of a function is the integral of a
  function with the interval of integration suitably translated.
\item The integral of a multiplicative transform of a function is the
  integral of the function with the interval of integration
  transformed by the same multiplicative factor, scaled by that
  multiplicative factor.
\end{enumerate}

\subsection{Symmetry and integration}

Words ...

\begin{enumerate}
\item If a function is continuous and even, its integral on $[-a,0]$
  equals its integral on $[0,a]$. More generally, its integrals on any
  two segments that are reflections of each other about the origin are
  equal. As a corollary, the integral on $[-a,a]$ is twice the
  integral on $[0,a]$.
\item If a function is continuous and odd, its integral on $[-a,0]$ is
  the negative of its integral on $[0,a]$. More generally, its
  integrals on any two segments that are reflections of each other
  about the origin are negatives of each other. As a corollary, the
  integral on $[-a,a]$ is zero.
\item If a function is continuous and has mirror symmetry about the
  line $x = c$, its integral on $[c-h,c]$ equals its integral on
  $[c,c+h]$.
\item If a function is continuous and has half-turn symmetry about
  $(c,f(c))$, its integral on any interval of the form $[c-h,c+h]$ is
  $2hf(c)$. Basically, all the variation about $f(c)$ {\em cancels
  out} and the {\em average value} is $f(c)$.
\item Suppose $f$ is continuous and periodic with period $h$ and $F$
  is an antiderivative of $f$. The integral of $f$ over any interval
  of length $h$ is constant. Thus, $F(x + h) - F(x)$ is the same
  constant for all $x$. (We saw this fact long ago, without proof).
\item The constant mentioned above is zero iff $F$ is periodic, i.e.,
  $f$ has a periodic antiderivative.
\item There is thus a well-defined {\em average value} of a continuous
  periodic function on a period. This is also the average value of the
  same periodic function on any interval whose length is a nonzero
  integer multiple of the period. This is also the limit of the
  average value over very large intervals.
\end{enumerate}

Actions...

\begin{enumerate}
\item All this even-odd-periodic stuff is useful for trivializing some
  integral calculations without computing antiderivatives. This is
  more than an idle observation, since in a lot of real-world
  situations, we get functions that have some obvious symmetry, even
  though we know very little about the concrete form of the
  functions. We use this obvious symmetry to compute the integral.
\item Even if the whole integrand does not succumb to the lure of
  symmetry, it may be that the integrand can be written as (something
  nice and symmetric) + (something computable). The (nice and
  symmetric) part is then tackled using the ideas of symmetry, and the
  computable part is computed.
\end{enumerate}

\subsection{Mean-value theorem}

Words ...

\begin{enumerate}
\item The {\em average value}, or {\em mean value}, of a continuous
  function on an interval is the quotient of the integral of the
  function on the interval by the length of the interval.
\item The mean value theorem for integrals says that a continuous
  function must attain its mean value somewhere on the interior of the
  interval.
\item For periodic functions, the mean value over any interval whose
  length is a multiple of the period is the same. Also, the mean value
  over a very large interval approaches this value.
\item The mean value of a periodic continuous function $f$ being $0$
  means that $f$ has a periodic antiderivative; in fact, every
  antiderivative of $f$ is periodic. If the mean value is nonzero,
  every antiderivative of $f$ is periodic with shift, and the linear
  part of any antiderivative has slope equal to the mean value of $f$.
\end{enumerate}

Actions ...

\begin{enumerate}
\item The mean values (over a period) of $\sin$ and $\cos$ are
  $0$. For $\sin$, we can see this since it is an {\em odd} periodic
  function. $\cos$ is a translate of $\sin$, hence has the same mean
  value.
\item The mean values (over a period) of $\sin^2$ and $\cos^2$ are
  $1/2$. The functions add up to $1$, and are translates of each
  other, so this makes sense.
\item The mean value (over a period) of $x \mapsto f(mx)$, $m \ne 0$,
  is the same as the mean value of $f$, where $f$ is a continuous
  periodic function.
\item The mean value (over a period) of $|\sin|$ is $2/\pi$, and that
  of $\sin^+$ is $1/\pi$.
\end{enumerate}

\subsection{Application to area computations}

Words ...

\begin{enumerate}

\item We can use integration to determine the area of the region
  between the graph of a function $f$ and the $x$-axis from $x = a$ to
  $x = b$: this integral is $\int_a^b f(x) \, dx$. The integral
  measures the signed area: parts where $f \ge 0$ make positive
  contributions and parts where $f \le 0$ make negative
  contributions. The magnitude-only area is given as $\int_a^b |f(x)|
  \, dx$. The best way of calculating this is to split $[a,b]$ into
  sub-intervals such that $f$ has constant sign on each sub-interval,
  and add up the areas on each sub-interval.
\item Given two functions $f$ and $g$, we can measure the area between
  $f$ and $g$ between $x = a$ and $x = b$ as $\int_a^b |f(x) - g(x)|
  \,dx$. For practical purposes, we divide into sub-intervals so that
  on each sub-interval one function is bigger than the other. We then
  use integration to find the magnitude of the area on each
  sub-interval and add up. If $f$ and $g$ are both continuous, the
  points where the functions {\em cross} each other are points where
  $f = g$.
\end{enumerate}

Actions ...

\begin{enumerate}
\item In some situations we are directly given functions and/or curves
  and are asked to find areas. In others, we are given real-world
  situations where we need to find areas of regions. Here, we have to
  find functions and set up the integration problem as an intermediate
  step.
\item In all these situations, it is important to draw the graphs in a
  reasonably correct way. This brings us to all the ideas that are
  contained in graph drawing. Remember, here we may be interested in
  simultaneously graphing more than one function. Thus, in addition to
  being careful about each function, we should also correctly estimate
  where one function is bigger than the other, and find (approximately
  or exactly) the intersection points. (Go over the notes on
  graph-drawing, and some additional notes on graphing that weren't
  completely covered in class).
\item In some situations, we are asked to find the area(s) of
  region(s) bounded by the graphs of one, two, three, or more
  functions. Here, we first need to sketch the figure. Then, we need
  to find the interval of integration, and if necessary, split this
  interval into sub-intervals, such that on each sub-interval, we know
  exactly what integral we need to do. For instance, consider the
  region between the graphs of $\sin$, $\cos$, and the
  $x$-axis. Basically, the idea is to find, for all the vertical
  slices, the upper and lower limits of the slice.
\end{enumerate}

\section{Graphing and miscellanea on functions}

\subsection{Symmetry yet again}

Words...

\begin{enumerate}
\item All mathematics is the study of symmetry (well, not all).
\item One interesting kind of symmetry that we often see in the graph
  of a function is {\em mirror symmetry} about a vertical line. This
  means that the graph of the function equals its reflection about the
  vertical line. If the vertical line is $x = c$ and the function is
  $f$, this is equivalent to asserting that $f(x) = f(2c - x)$ for all
  $x$ in the domain, or equivalently, $f(c + h) = f(c - h)$ whenever
  $c + h$ is in the domain. In particular, the domain itself must be
  symmetric about $c$.
\item A special case of mirror symmetry is the case of an {\em even
  function}. An even function is a function with mirror symmetry about
  the $y$-axis. In other words, $f(x) = f(-x)$ for all $x$ in the
  domain. (Even also implies that the domain should be symmetric about $0$).
\item Another interesting kind of symmetry that we often see in the
  graph of a function is {\em half-turn symmetry} about a point on the
  graph. This means that the graph equals the figure obtained by
  rotating it by an angle of $\pi$ about that point. A point $(c,d)$
  is a point of half-turn symmetry if $f(x) + f(2c - x) = 2d$ for all
  $x$ in the domain. In particular, the domain itself must be
  symmetric about $c$. If $f$ is defined at $c$, then $d = f(c)$.
\item A special case of half-turn symmetry is an odd function, which
  is a function having half-turn symmetry about the origin.
\item Another symmetry is {\em translation symmetry}. A function is
  {\em periodic} if there exists $h > 0$ such that $f(x + h) = f(x)$
  for all $x$ in the domain of the function (in particular, the domain
  itself should be invariant under translation by $h$). If a smallest
  such $h$ exists, then such an $h$ is termed the period of $f$.
\item A related notion is that of a function that is {\em periodic
  with shift}. A function is periodic with shift if there exists $h >
  0$ and $k \in \R$ such that $f(x + h) - f(x) = k$ for all $x \in
  \R$. Note that if $k$ is nonzero, the function isn't periodic.

  If $f$ is differentiable for all real numbers, then $f'$ is periodic
  if and only if $f$ is periodic with shift. In particular, if $f'$ is
  periodic with period $h$, then $f(x + h) - f(x)$ is constant. If
  this constant value is $k$, then the graph of $f$ has a
  two-dimensional translational symmetry by $(h,k)$ and its multiples.

  A function that is periodic with shift can be expressed as the sum
  of a linear function (slope $k/h$) and a periodic function. The
  linear part represents the secular trend and the periodic part
  represents the seasonal variation.
\end{enumerate}

Derivative facts...

\begin{enumerate}
\item The derivative of an even function, if defined everywhere, is
  odd. Any antiderivative of an odd function is even.
\item The derivative of an odd function is even. Any antiderivative of
  an even function is an odd function plus a constant.
\item The derivative of a function with mirror symmetry has half turn
  symmetry about the corresponding $x$-value and has value $0$ at that
  $x$-value. (For a more detailed description of these, see the
  solutions to the November 12 whoppers).
\item Assuming that $f'$ is defined and does not change sign
  infinitely often on a neighborhood of $c$, we have that if $x = c$
  is an axis of mirror symmetry for the graph of $f$, then $c$ is a
  point of local extremum. The reason is that if $f$ is increasing on
  the immediate left, it must be decreasing on the immediate right,
  and similarly ...
\item Assuming that $f''$ is defined and does not change sign
  infinitely often on a neighborhood of $c$, we have that if
  $(c,f(c))$ is a point of half-turn symmetry for the graph of $f$,
  then it is also a point of inflection for the graph. The reason is
  that if $f$ is concave up on the immediate left, it must be concave
  down on the immediate right, and similarly ...
\item The conver statements to the above two do not hold: most points
  of inflection do not give points of half-turn symmetry, and most
  local extrema do not give axes of mirror symmetry.
\item If $f$ has more than one axis of mirror symmetry, then it is
  periodic. Conversely, if $f$ is periodic with period $h$, and has an
  axis of mirror symmetry $x = c$, then all $x = c + (nh/2)$, $n$ an
  integer, are axes of mirror symmetry.
\item If $f$ has more than one point of half-turn symmetry, then it is
  periodic with shift. Conversely, if $f$ is periodic with shift and
  has a point of half-turn symmetry, it has infinitely many points of
  half-turn symmetry.
\end{enumerate}

Cute facts...

\begin{enumerate}
\item Constant functions enjoy mirror symmetry about every vertical
  line and half-turn symmetry about every point on the graph.
\item Nonconstant linear functions enjoy half-turn symmetry about
  every point on their graph. They do not enjoy any mirror symmetry
  (in the sense of mirror symmetry about vertical lines) because they
  are everywhere increasing or everywhere decreasing. (They do have
  mirror symmetry about {\em oblique} lines, but this is not a kind of
  symmetry that we are considering).
\item Quadratic (nonlinear) functions enjoy mirror symmetry about the
  line passing through the vertex (which is the unique absolute
  maximum/minimum, depending on the sign of the leading
  coefficient). They do not enjoy any half-turn symmetry.
\item Cubic functions enjoy half-turn symmetry about the point of
  inflection, and no mirror symmetry. Either the first derivative does
  not change sign anywhere, or it becomes zero at exactly one point,
  or there is exactly one local maximum and one local minimum,
  symmetric about the point of inflection.
\item Functions of higher degree do not necessarily have either
  half-turn symmetry or mirror symmetry.
\item More generally, we can say the following for sure: a nonconstant
  polynomial of even degree greater than zero can have at most one
  line of mirror symmetry and no point of half-turn symmetry. A
  nonconstant polynomial of odd degree greater than one can have at
  most one point of half-turn symmetry and no line of mirror symmetry.
\item The sine function is an example of a function where the points
  of inflection and the points of half-turn symmetry are the same: the
  multiples of $\pi$. Similarly, the points with vertical axis of
  symmetry are the same as the points of local extrema: odd multiples
  of $\pi/2$.
\item A polynomial is an even function iff all its terms have even
  degree. Such a polynomial is termed an {\em even polynomial}. A
  polynomial is an odd function iff all its terms have odd
  degree. Such a polynomial is termed an {\em odd polynomial}.
\end{enumerate}

Actions ...

\begin{enumerate}
\item Worried about periodicity? Don't be worried if you only see
  polynomials and rational functions. Trigonometric functions should
  make you alert. Try to fit in the nicest choices of period. Check if
  smaller periods can work (e.g., for $\sin^2$, the period is
  $\pi$). Even if the function in and of itself is not periodic, it
  might have a periodic derivative or a periodic second
  derivative. The sum of a linear function and a periodic function has
  periodic derivative, and the sum of a quadratic function and a
  periodic function has a periodic second derivative.
\item Want to milk periodicity? Use the fact that for a periodic
  function, the behavior everywhere is just the behavior over one
  period translates over and over again. If the first derivative is
  periodic, the increase/decrease behavior is periodic. If the second
  derivative is periodic, the concave up/down behavior is periodic.
\item Worried about even and odd, and half-turn symmetry and mirror
  symmetry? If you are dealing with a quadratic polynomial, or a
  function constructed largely from a quadratic polynomial, you are
  probably seeing some kind of mirror symmetry. For cubic polynomials
  and related constructions, think half-turn symmetry.
\item Use also the cues about even and odd polynomials.
\end{enumerate}

\subsection{Graphing a function}

Actions ...

\begin{enumerate}
\item To graph a function, a useful first step is finding the domain
  of the function.
\item It is useful to find the intercepts and plot a few additional points.
\item Try to look for symmetry: even, odd, periodic, mirror symmetry,
  half-turn symmetry, and periodic derivative.
\item Compute the derivative. Use that to find the critical points,
  the local extreme values, and the intervals where the function
  increases and decreases.
\item Compute the second derivative. Use that to find the points of
  inflection and the intervals where the function is concave up and
  concave down.
\item Look for vertical tangents and vertical cusps. Look for vertical
  asymptotes and horizontal asymptotes. For this, you may need to
  compute some limits.
\item Connect the dots formed by the points of interest. Use the
  information on increase/decrease and concave up/down to join these
  points. To make your graph a little better, compute the first
  derivative (possibly one-sided) at each of these points and start
  off your graph appropriately at that point.
\end{enumerate}

Subtler points...

\begin{enumerate}
\item When graphing a function, there may be many steps where you need
  to do some calculations and solve equations and you are unable to
  carry them out effectively. You can skip some of the steps and come
  back to them later.
\item If you cannot solve an equation exactly, try to approximate the
  locations of roots using the intermediate value theorem or other
  results such as Rolle's theorem.
\item In some cases, it is helpful to graph multiple functions
  together, on the same graph. For instance, we may be interested in
  graphing a function and its second and higher derivatives. There are
  other examples, such as graphing a function and its translates, or a
  function and its multiplicative shifts.
\item A graph can be used to suggest things about a function that are
  not obvious otherwise. However, the graph should not be used as
  conclusive evidence. Rather, the steps used in drawing the graph
  should be retraced and used to give an algebraic proof.
\item We are sometimes interested in sketching curves that are not
  graphs of functions. This can be done by locally expressing the
  curve piecewise as the graph of a function. Or, we could use many
  techniques similar to those for graphing functions.
\item For a function with a piecewise description, we plot each piece
  within its domain. At the points where the definition changes,
  determine the one-sided limits of the function and its first and
  second derivatives. Use this to make the appropriate open circles,
  asymptotes, etc.
\end{enumerate}

\section{Tricky topics}

\subsection{Piecewise definition by interval: new issues}

Before looking at these, please review the corresponding material on
piecewise definition by interval in the previous midterm review sheet.

\begin{enumerate}
\item Composition involving piecewise definitions is tricky. The
  limit, continuity and differentiation theorems for composition do
  not hold for one-sided approach. If one of the functions is
  decreasing, then things can get flipped. For piecewise definitions,
  when composing, we need to think clearly about how the intervals
  transform.

  Please review the midterm question on composition (midterm 1,
  question 7) of piecewise definitions. The key idea is as follows:
  for the composition $f \circ g$, we make cases to determine the
  values of $g$ for which the image under $g$ would land in a
  particular piece for the definition of $f$. Considering all cases is
  extremely painful and we are usually able to take shortcuts based on
  the nature of the problem.
\item For a function with piecewise definition, the points where the
  definition changes are endpoints for each definition, and hence,
  these points are possible candidates for critical points, points of
  inflection, and local extreme. They're just {\em candidates} (so
  they may not be any of these) but they're worth checking out.
\item A related helpful concept is that of {\em how smoothly} a
  function transitions at a point where its definition changes. 
\item At the one extreme are the discontinuous transitions, where the
  function has a non-removable discontinuity at the point. Such a
  transition may be a jump discontinuity (if both one-sided limits are
  defined but unequal) or something even worse, such as an infinite or
  oscillatory discontinuity.

  For functions with a discontinuity at a point, it makes sense to
  talk of one-sided derivatives only from the side where the function
  is continuous; of course, this one-sided derivative may still not
  exist.
\item A somewhat smoother transition occurs where the function is
  continuous but not differentiable at the point where it changes
  definition. This is a particular kind of {\em critical point} for
  the function definition. Critical points could arise in the form of
  vertical tangents, vertical cusps, or just plain points of turning
  such as for $|x|$ or $x^+$ at $x = 0$. At such points, it makes
  sense to try to compute the one-sided derivatives, and these can be
  computed just by differentiating the piece functions and plugging in
  at the point. The second derivative does not exist at such
  points. Also, there is an abrupt change in the nature of concavity
  at these points.
\item An even smoother transition occurs if the first derivative is
  defined at the point. If the first derivative is also defined around
  the point, then we can start thinking about the second derivative.
\item More generally, we could think of situations where we want the
  first $k$ derivatives to be defined at or around the point.
\item To integrate a function with a piecewise definition, partition
  the interval of integration in a manner that each part lies within
  one definition piece. Please review the following two routine
  problems from Homework 6: Exercise 5.4.55 and 5.4.60. You might want
  to do a few more suggested problems of the same type.
\end{enumerate}

\subsection{The $\sin(1/x)$ examples}

\begin{enumerate}
\item The $\sin(1/x)$ and related examples are somewhat tricky because
  the function definition differs at an {\em isolated point}, namely $0$.
\item To calculate any limit or derivative at a point other than $0$,
  we can do formal computations. However, to calculate the derivative
  at $0$, we {\em must} use the definition of derivative as a limit of
  a difference quotient.
\item For all the facts below, the qualitative conclusions at finite
  places hold if we replace $\sin$ by $\cos$. Those at $\infty$ change
  qualitatively.
\item The function $f_0(x) := \lbrace \begin{array}{rl} \sin(1/x), & x
  \ne 0 \\ 0, & x = 0 \\\end{array}$ is odd and satisfies the intermediate
  value property but is not continuous at $0$. Its limit at $\pm
  \infty$ is $0$, i.e., it has horizontal asymptote the $x$-axis in
  both directions.
\item The function $f_1(x) := \lbrace \begin{array}{rl} x\sin(1/x), &
  x \ne 0 \\ 0, & x = 0 \\\end{array}$ is even and continuous but not
  differentiable at $0$. We can see this from the pinching theorem --
  it is pinched between $|x|$ and $-|x|$. $f_1$ is infinitely
  differentable at all points other than $0$. Its limit at $\pm
  \infty$ is $1$, and it approaches this from below in both
  directions.
\item The function $f_2(x) := \lbrace \begin{array}{rl} x^2\sin(1/x), &
  x \ne 0 \\ 0, & x = 0 \\\end{array}$ is differentiable at $0$, and
  infinitely differentiable everywhere other than $0$, but the
  derivative is not continuous at $0$. The limit $\lim_{x \to 0}
  f_2'(x)$ does not exist. Note that $f_2'$ is defined everywhere and
  satisfies the intermediate value property but is not continuous at $0$.

  $f_2$ is asymptotic to the line $y = x$ both additively and
  multiplicatively, as $x \to \pm \infty$.

\item The function $f_3(x) := \lbrace \begin{array}{rl} x^3\sin(1/x),
  & x \ne 0 \\ 0, & x = 0 \\\end{array}$ is continuously
  differentiable but not twice differentiable at $0$, and infinitely
  differentiable everywhere other than $0$.

  $f_3$ is asymptotic to the line $y = x^2 + C$ as $x \to \pm \infty$,
  where $C$ is an actual constant (whose value you were supposed to
  compute in a homework problem).

\item More generally, consider something such as $p(x)
  \sin(1/(q(x)))$. This function is not defined at the zeros of
  $q$. However, it does not have vertical asymptotes at these
  points. If $a$ is a root of $q$ and also of $p$, then the limiting
  value as $x \to a$ is $0$. Otherwise, the limit is undefined but the
  function oscillates between finite bounds.

  In the limit as $x \to \pm \infty$, if the degree of $p$ is less
  than that of $q$, the function has horizontal asymptote the
  $y$-axis. If their degrees are equal, it has asymptote a finite
  nonzero value, namely $\lim_{x \to \infty} p(x)/q(x)$. If the degree
  of $p$ is bigger, it is asymptotic to a polynomial.

  For $p(x)\cos(1/(q(x)))$, the behavior at points where the function
  isn't defined is the same as for $\sin$, but the behavior at $\pm
  \infty$ is different -- the $\cos$ part goes to $1$, so the function
  is asymptotically polynomial, albeit not necessarily to $p$ itself.
\item Fun exercise: Consider $x\tan(1/x)$. What can you say about this?
\end{enumerate}

\subsection{Power functions}

We here consider exponents $r = p/q$, $q$ odd. When $q$ is even, or
when $r$ is irrational, the conclusions drawn here continue to hold
for $x > 0$; however, the function isn't defined for $x < 0$.

For each of these, you should be able to provide ready
justifications/reasoning based on derivatives.

\begin{enumerate}
\item Case $r < 0$: $x^r$ is undefined at $0$. It is decreasing and
  concave up on $(0,\infty)$, with vertical asymptote at $x = 0$ going
  to $+\infty$ and horizontal asymptote as $x \to \infty$ going to $y
  = 0$. If $p$ is even, it is increasing and concave up on
  $(-\infty,0)$ with horizontal asymptote as $x \to -\infty$ going to
  $y = 0$ and vertical asymptote $+\infty$ at $0$. If $p$ is odd, it
  is decreasing and concave down on $(-\infty,0)$ with horizontal
  asymptote as $x \to -\infty$ going to $y = 0$ and vertical asymptote
  $-\infty$ at $0$.
\item Case $r = 0$: We get a constant function with value $1$.
\item Case $0 < r < 1$: $x^r$ is increasing and concave down on
  $(0,\infty)$. If $p$ is even, it is decreasing and concave down on
  $(-\infty,0)$ and has a downward-facing vertical cusp at $(0,0)$. If
  $p$ is odd, it is increasing and concave up on $(-\infty,0)$ and has
  an upward vertical tangent at $(0,0)$.
\item Case $r = 1$: A straight line $y = x$.
\item Case $1 < r$: $x^r$ is increasing and concave up on
  $(0,\infty)$. If $p$ is even, it is decreasing and concave up on
  $(-\infty,0)$ and has a local and absolute minimum and critical
  point at $(0,0)$. If $p$ is odd, it is increasing and concave down
  on $(-\infty,0)$ and has a point of inflection-type critical point
  (no local extreme value) at $(0,0)$.
\end{enumerate}

\subsection{Local behavior heuristics: multiplicative}

You have a complicated looking function such that $(x -
\alpha_1)^{r_1}(x - \alpha_2)^{r_2} \dots (x - \alpha_k)^{r_k}$. What
is the local behavior of the function near $x = \alpha_1$?

The answer: For determining the qualitative nature of this local
behavior, you can just concentrate on $(x - \alpha_i)^{r_i}$ and
ignore the rest. In particular, {\em just} looking at $r_1$, you can
figure out whether you have a critical point, local extremum, point of
inflection, vertical tangent, or vertical cusp. The other things {\em
do} matter if you are further interested in, say, whether we have a
local maximum or minimum, or in whether the vertical tangent is an
increasing tangent or a decreasing tangent, or which direction the
vertical cusp points in.

Overall, if $r_i > 0$ and $r_i = p_i/q_i$, $q_i$ odd, and both $p_i$,
$q_i$ positive, then:

\begin{itemize}
\item Critical point iff $r_i \ne 1$.
\item Local extremum iff $p_i$ even.
\item Point of inflection/vertical tangent iff $p_i$ odd.
\item Vertical cusp iff $p_i$ even and $p_i < q_i$, both positive,
  i.e., $r_i < 1$. Note that vertical cusp is a special kind of local extremum.
\item Vertical tangent iff $p_i$ odd, $p_i < q_i$, both positive,
i.e., $r_i < 1$.
\end{itemize}

So, if we look at, say $(x - \pi)^2(x - \sqrt{6})^3(x - 2)^{1/3}(x -
3)^{2/3}$, it has a local extremum at $\pi$, a point of inflection at
$\sqrt{6}$, a vertical tangent at $2$, and a vertical cusp at $3$.

\subsection{Local behavior heuristics: additive}

If you have something of the form $f + g$, and the vertical
tangent/cusp points for $f$ are disjoint from those of $g$, then the
vertical tangent/cusp points for $f + g$ include both lists. Further,
the nature (tangent versus cusp) is inherited from the corresponding
piece.

For instance, for $x^{1/3} + (x - 131.4)^{2/3}$, there is a vertical
tangent at $x = 0$ and a vertical cusp at $x = 131.4$.

In particular, if $g$ is everywhere differentiable, then the vertical
tangent/cusp behavior of $f + g$ is the same as that of $f$.

\section{High yield practice}

Here are the areas that you should focus on if you have a thorough grasp of the basics:

\begin{enumerate}
\item Everything to do with piecewise definitions (differentiation,
  integration, reasoning).
\item Vertical tangents and cusps in sophisticated cases.
\item Horizontal, oblique, and weird asymptotes.
\item Trigonometric integrations, particularly $\sin^2$, $\cos^2$, and
  $\tan^2$ and their variants.
\item Tricky integration problems that involve the use of symmetry
  and/or the chain rule.
\end{enumerate}

\section{Quickly}

This ``Quickly'' list is a bit of a repeat and augmentation of the
``Quicky'' list given out for the previous midterm.

\subsection{Arithmetic}

You should be able to:

\begin{itemize}
\item Do quick arithmetic involving fractions.
\item Remember $\sqrt{2}$, $\sqrt{3}$, and $\pi$ to at least two
  digits.
\item Sense when an expression will simplify to $0$.
\item Compute approximate values for square roots of small numbers,
  $\pi$ and its multiples, etc., so that you are able to figure out,
  for instance, whether $\pi/4$ is smaller or bigger than $1$, or two
  integers such that $\sqrt{39}$ is between them.
\item Know or quickly compute small powers of small positive
  integers. This is particularly important for computing definite
  integrals. For instance, to compute $\int_2^3 (x + 1)^3 \, dx$, you
  need to know/compute $3^4$ and $4^4$.
\end{itemize}

\subsection{Computational algebra}

You should be able to:

\begin{enumerate}
\item Add, subtract, and multiply polynomials.
\item Factorize quadratics or determine that the quadratic cannot be
  factorized.
\item Factorize a cubic if at least one of its factors is a small and
  easy-to-spot number such as $0$, $\pm 1$, $\pm 2$, $\pm 3$.
\item Do polynomial long division (not usually necessary, but helpful).
\item Solve simple inequalities involving polynomial and rational
  functions once you've obtained them in factored form.
\end{enumerate}

\subsection{Computational trigonometry}

You should be able to:

\begin{enumerate}
\item Determine the values of $\sin$, $\cos$, and $\tan$ at multiples
  of $\pi/2$.
\item Determine the intervals where $\sin$ and $\cos$ are positive and
  negative.
\item Remember the formulas for $\sin(\pi - x)$ and $\cos(\pi - x)$,
as well as formulas for $\sin(-x)$ and $\cos(-x)$.
\item Recall the values of $\sin$ and $\cos$ at $\pi/6$, $\pi/4$, and
  $\pi/3$, as well as at the corresponding obtuse angles.
\item Reverse lookup for these, for instance, you should quickly
  identify the acute angle whose $\sin$ is $1/2$.
\end{enumerate}

\subsection{Computational limits}

You should be able to: size up a limit, determine whether it is of the
form that can be directly evaluated, of the form that we already know
does not exist, or indeterminate.

\subsection{Computational differentiation}

You should be able to:

\begin{enumerate}
\item Differentiate a polynomial (written in expanded form) on sight
  (without rough work).
\item Differentiate a polynomial (written in expanded form) twice
  (without rough work).
\item Differentiate sums of powers of $x$ on sight (without rough
  work).
\item Differentiate rational functions with a little thought.
\item Do multiple differentiations of expressions whose derivative
  cycle is periodic, e.g., $a \sin x + b \cos x$.
\item Differentiate simple composites without rough work (e.g.,
  $\sin(x^3)$).
\end{enumerate}

\subsection{Computational integration}

You should be able to:

\begin{enumerate}
\item Compute the indefinite integral of a polynomial (written in
  expanded form) on sight without rough work.
\item Compute the definite integral of a polynomial with very few
  terms within manageable limits quickly.
\item Compute the indefinite integral of a sum of power functions
  quickly.
\item Know that the integral of sine or cosine on any quadrant is $\pm
  1$.
\item Compute the integral of $x \mapsto f(mx)$ if you know how to
  integrate $f$. In particular, integrate things like $(a + bx)^m$.
\item Integrate $\sin$, $\cos$, $\sin^2$, $\cos^2$, $\tan^2$,
  $\sec^2$, $\cot^2$, $\csc^2$,.
\end{enumerate}
\subsection{Being observant}

You should be able to look at a function and:

\begin{enumerate}
\item Sense if it is odd (even if nobody pointedly asks you whether it
  is).
\item Sense if it is even (even if nobody asks you whether it is).
\item Sense if it is periodic and find the period (even if nobody asks
  you about the period).
\end{enumerate}

\subsection{Graphing}

You should be able to:

\begin{enumerate}
\item Mentally graph a linear function.
\item Mentally graph a power function $x^r$ (see the list of things to
  remember about power functions). Sample cases for $r$: $1/3$, $2/3$,
  $4/3$, $5/3$, $1/2$, $1$, $2$, $3$, $-1$, $-1/3$ $-2/3$.
\item Graph a piecewise linear function with some thought.
\item Mentally graph a quadratic function (very approximately) --
  figure out conditions under which it crosses the axis etc.
\item Graph a cubic function after ascertaining which of the cases for
  the cubic it falls under.
\item Mentally graph $\sin$ and $\cos$, as well as functions of the $A
  \sin(mx)$ and $A\cos(mx)$.
\item Graph a function of the form linear + trigonometric, after doing
  some quick checking on the derivative.
\end{enumerate}

\subsection{Fancy pictures}

Keep in mind approximate features of the graphs of:

\begin{enumerate}
\item $\sin(1/x)$, $x\sin(1/x)$, $x^2 \sin(1/x)$ and $x^3\sin(1/x)$,
  and the corresponding $\cos$ counterparts -- both the behavior near
  $0$ and the behavior near $\pm \infty$.
\item The Dirichlet function and its variants -- functions defined
  differently for the rationals and irrationals.
\end{enumerate}

\end{document}
