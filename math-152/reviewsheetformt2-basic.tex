\documentclass[10pt]{amsart}
\usepackage{fullpage,hyperref,vipul, graphicx}
\title{Review sheet for midterm 2: basic }
\author{Math 152, Section 55 (Vipul Naik)}

\begin{document}
\maketitle

Unlike the previous midterm, I've split the review sheet into two
parts: basic and advanced. The basic review sheet contains (with some
changes) the executive summaries from the lecture notes.

This is the {\em basic} part of the review sheet for midterm
2. Hopefully, you will not need to go through this more than once,
except the points that give you trouble the first time around.

The advanced part contains the error-spotting exercises, a section on
``Graphing and Miscellanea on Functions'' and a section on ``Tricky
Topics'' and this is what we will focus on in the live review
session. It also contains a ``Quickly'' list of stuff you should be
able to recall and do quickly.

Section numbers from the basic and advanced are matched up so that you
can look at both sheets together.

{\em This document does not re-review material already covered in the
review sheet for midterm 1. It is your responsibility to go through
that review sheet again and make sure you have mastered all the
material there.} We could cover in the review session some topics
there that you are having difficulty with, but that will not be a
priority.

\section{Left-overs from differentiation basics}

\subsection{Derivative as rate of change}

Words...

\begin{enumerate}
\item The derivative of $A$ with respect to $B$ is the rate of change
  of $A$ with respect to $B$. Thus, to determine rates of change of
  various quantities, we can use the techniques of differentiation.
\item If there are three linked quantities that are changing together
  (e.g., different measures for a circle such as radius, diameter,
  circumference, area) then we can use the chain rule.
\end{enumerate}

Most of the actions in this case are not more than a direct
application of the words.

\subsection{Implicit differentiation}

Words...

\begin{enumerate}
\item Suppose there is a curve in the plane, whose equation cannot be
  manipulated easily to write one variable in terms of the other. We
  can use the technique of implicit differentiation to determine the
  derivative, and hence the slope of the tangent line, at different
  points to the curve.
\item For a curve where neither variable is expressible as a function
  of the other, the notion of derivative still makes sense as long as
  {\em locally}, we can get $y$ as a function of $x$. For instance,
  for the circle $x^2 + y^2 = 1$, $y$ is not a function of $x$, but if
  we restrict attention to the part of the circle above the $x$-axis,
  then on this restricted region, $y$ is a function of $x$.
\item In some cases, even when one variable is expressible as a
  function of the other, implicit differentiation is easier to handle
  as it may involve fewer messy squareroot symbols.
\end{enumerate}

Actions ...

\begin{enumerate}
\item To determine the derivative using implicit differentiation,
  write down the equations of both curves, differentiate both sides
  with respect to $x$, and simplify using all the differentiation
  rules, to get everything in terms of $x$, $y$, and $dy/dx$. Isolate
  the $dy/dx$ term in terms of $x$ and $y$, and compute it at whatever
  point is needed.
\item This procedure can be iterated to compute higher order
  derivatives at specific points on the curve where the curve locally
  looks like a function.
\end{enumerate}

\section{Increase/decrease, maxima/minima, concavity, inflection, tangents, cusps, asymptotes}

\subsection{Rolle's, mean value, increase/decrease, maxima/minima}

Words...

\begin{enumerate}
\item If a function $f$ is continuous on the closed interval $[a,b]$
  and differentiable on the open interval $(a,b)$, and $f(a) = f(b) =
  0$, then there exists $c \in (a,b)$ such that $f'(c) = 0$. This is
  called {\em Rolle's theorem} and is a consequence of the
  extreme-value theorem.
\item If a function $f$ is continuous on the closed interval $[a,b]$
  and differentiable on the open interval $(a,b)$, then there exists
  $c \in (a,b)$ such that $f'(c)$ is the difference quotient $(f(b) -
  f(a))/(b - a)$. This result is called the {\em mean-value
  theorem}. Geometrically, it says that for any chord, there is a
  parallel tangent. Another way of thinking about it is that every
  difference quotient is equal to a derivative at some intermediate
  point.
\item If $f$ is a function and $c$ is a point such that $f(c) \ge
  f(x)$ for $x$ to the immediate left of $c$, we say that $c$ is a
  local maximum from the left. In this case, the left-hand derivative
  of $f$ at $c$, if it exists, is greater than or equal to zero. This
  is because the difference quotient is greater than or equal to
  zero. Local maximum from the right implies that the right-hand
  derivative (if it exists) is $\le 0$, local minimum from the left
  implies that the left-hand derivative (if it exists) is $\le 0$, and
  local minimum from the right implies that the right-hand derivative
  (if it exists) is $\ge 0$. Even in the case of {\em strict} local
  maxima and minima, we still need to retain the equality sign on the
  derivative because it occurs as a {\em limit} and a limit of
  positive numbers can still be zero.
\item If $c$ is a point where $f$ attains a local maximum (i.e., $f(c)
  \ge f(x)$ for all $x$ close enough to $c$ on both sides), then
  $f'(c)$, if it exists, is equal to zero. Similarly for local
  minimum.
\item A {\em critical point} for a function is a point where either
  the function is not differentiable or the derivative is zero. All
  local maxima and local minima must occur at critical points.
\item If $f'(x) > 0$ for all $x$ in the open interval $(a,b)$, $f$ is
  increasing on $(a,b)$. Further, if $f$ is one-sided continuous at
  the endpoint $a$ and/or the endpoint $b$, then $f$ is increasing on
  the interval including that endpoint. Similarly, $f'(x) < 0$ implies
  $f$ decreasing.
\item If $f'(x) > 0$ everywhere except possibly at some isolated
  points (so that they don't cluster around any point) where $f$ is
  still continuous, then $f$ is increasing everywhere.
\item If $f'(x) = 0$ on an open interval, $f$ is constant on that
  interval, and it takes the same constant value at an endpoint where
  it's continuous from the appropriate side.
\item If $f$ and $g$ are two functions that are both continuous on an
  interval $I$ and have the same derivative on the interior of $I$,
  then $f - g$ is a constant function.

  {\em Note: Due to an oversight, the remaining items in this list
  were not included in the executive summary to the lecture notes.}

\item There is a {\em first derivative test} which provides a
  sufficient (though not necessary) condition for a local extreme
  value: it says that if the first derivative is nonnegative
  (respectively positive) on the immediate left of a critical point,
  that gives a strict local maximum (respectively local maximum) from
  the left. If the first derivative is negative on the immediate left,
  we get a strict local minimum from the left. If the first derivative
  is positive on the immediate right, we get a strict local minimum
  from the right, and if it is negative on the immediate right, we get
  a strict local maximum from the right.

  The first derivative test is similar to the corresponding
  ``one-sided derivative'' test, but is somewhat stronger for a
  variety of situations because in many cases, one-sided derivatives
  are zero, which is inconclusive, whereas the first derivative test
  fails us more rarely.
\item The second derivative test states that if $f$ has a critical
  point $c$ where it is twice differentiable, then $f''(c) > 0$
  implies that $f$ has a local minimum at $c$, and $f''(c) < 0$
  implies that $f$ has a local maximum at $c$.
\item There are also higher derivative tests that work for critical
  points $c$ where $f'(c) = 0$. These work as follows: we look for the
  smallest $k$ such that $f^{(k)}(c) \ne 0$. If this $k$ is even, then
  $f$ has a local extreme value at $c$, and the nature (max versus
  min) depends on the sign of $f^{(k)}(c)$ (max if negative, min if
  positive). If $k$ is odd, then we have what we'll see soon is a
  point of inflection.
\item To determine absolute maxima/minima, the candidates are: points
  of discontinuity, boundary points of domain (whether included in
  domain or outside the domain; if the latter, then limiting),
  critical points (derivative zero or undefined), and limiting cases
  at $\pm \infty$.
\end{enumerate}

Actions... (think of examples that you've done)

\begin{enumerate}
\item Rolle's theorem, along with the more sophisticated formulations
  involving increasing/decreasing, tell us that there is an intimate
  relationship between the zeros of a function and the zeros of its
  derivative. Specifically, between any two zeros of the function,
  there is a zero of its derivative. Thus, if a function has $r$
  zeros, the derivative has at least $r - 1$ zeros, with at least one
  zero between any two consecutive zeros of $f$.
\item The more sophisticated version tells us that between any two
  zeros of a differentiable function, the function must attain a local
  maximum or local minimum. So, if the function is increasing
  everywhere or decreasing everywhere, there is at most one zero.
\item The mean-value theorem allows us to use bounds on the derivative
  of a function to bound the overall variation, or change, in the
  function. This is because if the derivative cannot exceed some
  value, then the difference quotient also cannot exceed that value,
  which means that the function cannot change too quickly on average.
\item To determine regions where a function is increasing and
  decreasing, we find the derivative and determine regions where the
  derivative is positive, zero, and negative.
\item To determine all the local maxima and local minima of a
  function, find all the critical points. To find the critical points,
  solve $f' = 0$ and also consider, as possible candidates, all the
  points where the function changes definition. {\em Although a point
  where the function changes definition need not be a critical point,
  it is a very likely candidate.}
\item {\em This item was missed in the original executive summary}: To
  determine absolute maxima and absolute minima, find all candidates
  (discontinuity, endpoints, limiting cases, boundary points),
  evaluate at each, and compare. Note that any absolute maximum must
  arise as a local or endpoint maximum. However, instead of first
  determining which critical points give local maxima by the
  derivative tests, we can straightaway compute values everywhere and
  compare, if our interest is solely in finding the absolute maximum
  and minimum.
\end{enumerate}

\subsection{Concave up/down and points of inflection}

Words ...

\begin{enumerate}
\item A function is called {\em concave up} on an interval if it is
  continuous and its first derivative is continuous and increasing on
  the interval. If the function is twice differentiable, this is
  equivalent to requiring that the second derivative be positive
  except possibly at isolated points, where it can be zero. (Think
  $x^4$, whose first derivative, $4x^3$, is increasing, and the second
  derivative is positive everywhere except at $0$, where it is zero).
\item A function is called {\em concave down} on an interval if it is
  continuous and its first derivative is continuous and decreasing on
  the interval. If the function is twice differentiable, this is
  equivalent to requiring that the second derivative be negative
  except possibly at isolated points, where it can be zero.
\item A {\em point of inflection} is a point where the sense of
  concavity of the function changes. A point of inflection for a
  function is a point of local extremum for the first derivative.
\item Geometrically, at a point of inflection, the tangent line to the
  graph of the function {\em cuts through} the graph.
\end{enumerate}

Actions ...

\begin{enumerate}

\item To determine points of inflection, we first find critical points
  for the first derivative (which are points where this derivative is
  zero or undefined) and then use the first or second derivative test
  at these points. Note that these derivative tests are applied to
  the first derivative, so the first derivative here is the second
  derivative and the second derivative here is the third derivative.
\item In particular, if the second derivative is zero and the third
  derivative exists and is nonzero, we have a point of inflection.
\item A point where the first two derivatives are zero could be a
  point of local extremum or a point of inflection. To find out which
  one it is, we either use sign changes of the derivatives, or we use
  higher derivatives.
\item Most importantly, the second derivative being zero does {\em
  not} automatically imply that we have a point of inflection.
\item If the third derivative is zero, we can use a higher derivative
  test again. The upshot is that if the first $k \ge 2$ for which
  $f^{(k)}(c) \ne 0$ is even, then we do not have a point of
  inflection, but if the first $k$ is odd, then we have a point of
  inflection.
\end{enumerate}

\subsection{Tangents, cusps, and asymptotes}

Words...

\begin{enumerate}

\item We say that $f$ has a horizontal asymptote with value $L$ if
  $\lim_{x \to \infty} f(x) = L$ or $\lim_{x \to -\infty} f(x) =
  L$. Sometimes, both might occur. (In fact, in almost all the
  examples you have seen, the limits at $\pm \infty$, if finite, are
  both equal).
\item We say that $f$ has a vertical asymptote at $c$ if $\lim_{x \to
  c^-} f(x) = \pm \infty$ and/or $\lim_{x \to c^+} f(x) = \pm
  \infty$. Note that in this case, it usually also happens that $f'(x)
  \to \pm \infty$ on the relevant side, with the sign the same as that
  of $f(x)$'s approach if the approach is from the left and opposite
  to that of $f(x)$'s approach if the approach is from the
  right. However, this is not a foregone conclusion.
\item We say that $f$ has a vertical tangent at the point $c$ if $f$
  is continuous (and finite) at $c$ and $\lim_{x \to c} f'(x) = \pm
  \infty$, with the {\em same sign of infinity} from both sides. If
  $f$ is increasing, this sign is $+\infty$, and if $f$ is decreasing,
  this sign is $-\infty$. Geometrically, points of vertical tangent
  behave a lot likepoints of inflection and usually {\em do} give
  points of inflection (in the sense that the tangent line cuts
  through the graph). Think $x^{1/3}$.
\item We say that $f$ has a vertical cusp at the point $c$ if $f$ is
  continuous (and finite) at $c$ and $\lim_{x \to c^-} f'(x)$ and
  $\lim_{x \to c^+} f'(x)$ are infinities of opposite sign. In other
  words, $f$ takes a sharp about-turn at the $x$-value of $c$. Think
  $x^{2/3}$.
\item We say that $f$ is asymptotic to $g$ if $\lim_{x \to \infty}
  f(x) - g(x) = \lim_{x \to -\infty} f(x) - g(x) = 0$. In other words,
  the graphs of $f$ and $g$ come progressively closer as $|x|$ becomes
  larger. (We can also talk of one-sided asymptoticity, i.e.,
  asymptotic only in the positive direction or only in the negative
  direction). When $g$ is a {\em nonconstant linear function}, we say
  that $f$ has an {\em oblique asymptote}. Horizontal asymptotes are a
  special case, where one of the functions is a constant function.
\end{enumerate}

Actions...

\begin{enumerate}
\item Finding the horizontal asymptotes involves computing limits as
  the domain value goes to infinity. Finding the vertical asymptotes
  involves locating points in the domain, or the boundary of the
  domain, where the function limits off to infinity. For both of
  these, it is useful to remember the various rules for limits related
  to infinities.
\item Remember that for a vertical tangent or vertical cusp at a
  point, it is necessary that the function be continuous (and take a
  finite value). So, we not only need to find the points where the
  derivative goes off to infinity, we also need to make sure those are
  points where the function is continuous. Thus, for the function
  $f(x) = 1/x$, $f'(x) \to - \infty$ on both sides as $x \to 0$, but
  we do {\em not} obtain a vertical tangent -- rather, we obtain a
  vertical asymptote.
\end{enumerate}

\section{Max-min problems}

Words...

\begin{enumerate}
\item In real-world situations, maximization and minimization problems
  typically involve multiple variables, multiple constraints on those
  variables, and some objective function that needs to be maximized or
  minimized.
\item The only thing we know to solve such problems is to reduce
  everything in terms of one variable. This is typically done by {\em
  using up} some of the constraints to express the other variables in
  terms of that variable.
\item The problem then typically boils down to a
  maximization/minimization problem of a function in a single variable
  over an interval. We use the usual techniques for understanding this
  function, determining the local extreme values, determining the
  endpoint extreme values, and determining the absolute extreme
  values.
\end{enumerate}

{\em Special note: For integer optimization, please refer back to the
lecture notes!}

Actions... (think of examples; also review the notes on max-min problems)

\begin{enumerate}
\item Extremes sometimes occur at endpoints but these endpoints could
  correspond to degenerate cases. For instance, of all the rectangles
  with given perimeter, the square has the maximum area, and the
  minimum occurs in the degenerate case of a rectangle where one side
  has length zero.
\item Some constraints on the variables we have are explicitly stated,
  while others are implicit. Implicit constraints include such things
  as nonnegativity constraints. {\em Some of these implicit
  constraints may be on variables other than the single variable in
  terms of which we eventually write everything.}
\item After we have obtained the objective function in terms of one
  variable, we are in a position to throw out the other
  variables. However, before doing so, it is {\em necessary to
  translate all the constraints into constraints on the one variable
  that we now have}. 
\item When our intent is to maximize a function, it is sometimes
  useful to maximize an equivalent function that is easier to
  visualize or differentiate. For instance, to maximize $\sqrt{f(x)}$
  is equivalent to maximizing $f(x)$ if $f(x)$ is nonnegative. With
  this way of thinking about equivalent functions, we can make sure
  that the actual function that we differentiate is easy to
  differentiate. The main criterion is that the two functions should
  rise and fall together. (Analogous observations apply for
  minimizing) Remember, however, that to calculate the {\em value} of
  the maximum/minimum, you should go back to the original function.
\item Sometimes, there are other parameters in the
  maximization/minimization problem that are {\em unknown constants},
  and the final solution is expected to be in terms of those
  constants. In rare cases, the nature of the function, and hence the
  nature of maxima and minima, depends on whether those constants fall
  in particular intervals. {\em If you find this to be the case, go
  back to the original problem and see whether the real-world
  situation it came from constrains the constants to one of the
  intervals}.
\item For some geometrical problems, the maximization/minimization can
  be done trigonometrically. Here, we make a clever choice of an angle
  that controls the {\em shape} of the figure and then use the
  trigonometric functions of that angle. This could provide alternate
  insight into maximization.
\end{enumerate}

\section{Definite and indefinite integration}

\subsection{Definition and basics}

Words ...

\begin{enumerate}
\item The definite integral of a continuous (though somewhat weaker
  conditions also work) function $f$ on an interval $[a,b]$ is a measure
  of the signed area between the graph of $f$ and the $x$-axis. It
  measures the {\em total value} of the function.
\item For a partition $P$ of $[a,b]$, the lower sum $L_f(P)$ adds up,
  for each subinterval of the partition, the length of that interval
  times the minimum value of $f$ over that interval. The upper sum
  adds up, for each subinterval of the partition, the length of that
  interval times the maximum value of $f$ on that subinterval.
\item Every lower sum of $f$ is less than or equal to every upper sum
  of $f$.
\item The {\em norm} or {\em size} of a partition $P$, denoted $\| P
  \|$, is defined as the maximum of the lengths of its
  subintervals.
\item If $P_1$ is a finer partition than $P_2$, i.e., every interval
  of $P_1$ is contained in an interval of $P_2$, then the following
  three things are true: (a) $L_f(P_2) \le L_f(P_1)$, (b) $U_f(P_1)
  \le U_f(P_2)$, and (c) $\| P_1 \| \le \| P_2 \|$.
\item If $\lim_{\| P \| \to 0} L_f(P) = \lim_{\| P \| \to 0} U_f(P)$,
  then this common limit is termed the {\em integral} of $f$ on the
  interval $[a,b]$.
\item We can define $\int_a^b f(x) \, dx$ as above if $a < b$. If $a =
  b$ the integral is defined to be $0$. If $a > b$, the integral is
  defined as $-\int_b^a f(x) \, dx$.
\item A continuous function on $[a,b]$ has an integral on $[a,b]$. A
  piecewise continuous function where one-sided limits exist and are
  finite at every point is also integrable.
\end{enumerate}

Actions ...

\begin{enumerate}
\item For constant functions, the integral is just the product of the
  value of the function and the length of the interval.
\item Points don't matter. So, if we change the value of a function at
  one point while leaving the other values unaffected, the integral
  does not change.
\item A first-cut lower and upper bound on the integral can be
  obtained using the {\em trivial} partition, where we do not
  subdivide the interval at all. The upper bound is thus the maximum
  value times the length of the interval, and the lower bound is the
  minimum value times the length of the interval.
\item The finer the partition, the closer the lower and upper bounds,
  and the better the approximation we obtain for the integral.
\item A very useful kind of partition is a {\em regular partition},
  which is a partition where all the parts have the same length. If
  the integral exists, we can calculate the actual integral as
  $\lim_{n \to \infty}$ of the upper sums or the lower sums for a
  regular partition into $n$ parts.
\item When a function is increasing on some parts of the interval and
  decreasing on other parts, it is useful to choose the partition in
  such a way that on each piece of the partition, the function is
  either increasing throughout or decreasing throughout. This way, the
  maximum and minimum occur at the endpoints in each piece. In
  particular, try to choose all points of local extrema as points of
  partition.
\end{enumerate}

\subsection{Definite integral, antiderivative, and indefinite integral}

Words ..

\begin{enumerate}
\item We have $\int_a^b f(x) \, dx + \int_b^c f(x) \, dx = \int_a^c
  f(x) \, dx$.
\item We say that $F$ is an antiderivative for $f$ if $F' = f$.
\item For a continuous function $f$ defined on a closed interval
  $[a,b]$, and for a point $c \in [a,b]$, the function $F$ given by
  $F(x) = \int_c^x f(t) \, dt$ is an antiderivative for $f$.
\item If $f$ is continuous on $[a,b]$ and $F$ is a function continuous
  on $[a,b]$ such that $F' = f$ on $(a,b)$, then $\int_a^b f(x) \, dx
  = F(b) - F(a)$.
\item The two results above essentially state that differentiation and
  integration are opposite operations.
\item For a function $f$ on an interval $[a,b]$, if $F$ and $G$ are
  antiderivatives, then $F - G$ is constant on $[a,b]$. Conversely, if
  $F$ is an antiderivative of $f$, so is $F$ plus any constant.
\item The {\em indefinite integral} of a function $f$ is the
  collection of all antiderivatives for the function. This is
  typically written by writing one antiderivative plus $C$, where $C$
  is an arbitrary constant. We write $\int f(x) \, dx$ for the
  indefinite integral. Note that there are no upper and lower limits.
\item Both the definite and the indefinite integral are additive. In
  other words, $\int f(x) \, dx + \int g(x) \, dx = \int f(x) + g(x)
  \, dx$. The analogue holds for definite integrals, with limits.
\item We can also pull constants multiplicatively out of integrals.
\item Note the important formula:

  $$\frac{d}{dx} \int_{u(x)}^{v(x)} f(t) \, dt = f(v(x))v'(x) - f(u(x))u'(x)$$
\end{enumerate}

Actions ...

\begin{enumerate}
\item Consider the integration problem:

  $$\int \frac{2x}{x^3 + 1} \, dx = \int \frac{2x}{x^3} \, dx + \int \frac{2x}{1} \, dx = \int \frac{2}{x^2} \, dx + \int 2x \, dx = \frac{-2}{x} + x^2 + C$$
\item To do a definite integral, find any one antiderivative and
  evaluate it between limits.
\item An important caveat: when using antiderivatives to do a definite
  integral, it is important to make sure that the antiderivative is
  defined and continuous everywhere on the interval of
  integration. (Think of the $1/x^3$ example). 
\item To do an indefinite integral, find any antiderivative and put a
  $+ C$.
\item To find an antiderivative, use the additive splitting and
  pulling constants out, and the fact that $\int x^r \, dx = x^{r +
    1}/(r + 1)$.
\end{enumerate}

\subsection{Higher derivatives, multiple integrals, and initial/boundary conditions}

Actions ...

\begin{enumerate}
\item The simplest kind of {\em initial value problem} (a notion we
  will encounter again when we study differential equations) is as
  follows. The $k^{th}$ derivative of a function is given on the
  entire domain. Next, the {\em values} of the function and the first
  $k - 1$ derivatives are given at a single point of the domain. We
  can use this data to find the function. Step by step, we find
  derivatives of lower orders. First, we integrate the $k^{th}$
  derivative to get that the $(k-1)^{th}$ derivative is of the form
  $F(x) + C$, where $C$ is unknown. We now use the value of the
  $(k-1)^{th}$ derivative at the given point to find $C$. Now, we have
  the $(k-1)^{th}$ derivative. We proceed now to find the $(k-2)^{th}$
  derivative, and so on.
\item Sometimes, we may be interested in finding {\em all} functions
  with a given second derivative $f$. For this, we have to perform an
  indefinite integration twice. The net result will be a general
  expression of the form $F(x) + C_1x + C_2$, where $F$ is a function
  with $F'' = f$, and $C_1$ and $C_2$ are arbitrary constants. In
  other words, we now have {\em up to constants or linear functions}
  instead of {\em up to constants} as our degree of ambiguity.
\item More generally, if the $k^{th}$ derivative of a function is
  given, the function is uniquely determined up to additive
  differences of polynomials of degree strictly less than $k$ (so,
  degree at most $k - 1$). The number of free constants that can take
  arbitrary real values is $k$ (namely, the coefficients of the
  polynomial).
\item This general expression is useful if, instead of an initial
  value problem, we have a boundary value problem. Suppose we are
  given $G''$ as a function, and we are given the value of $G$ at two
  points. We can then first find the general expression for $G$ as $F
  + C_1x + C_2$. Next, we plug in the values to get a system of two
  linear equations, that we solve in order to determine $C_1$ and
  $C_2$, and hence $G$.
\end{enumerate}

\subsection{Reversing the chain rule}

Words ...

\begin{enumerate}
\item The chain rule states that $(f \circ g)' = (f' \circ g) \cdot
  g'$.
\item Some integrations require us to reverse the chain rule. For
  this, we need to realize the integrand that we have in the form of the
  right-hand side of the chain rule.
\item The first step usually is to find the correct function $g$,
  which is the {\em inner function} of the composition, then to adjust
  constants suitably so that the remaining term is $g'$, and then
  figure out what $f'$ is. Finally, we find an antiderivative for
  $f'$, which we can call $f$, and then compute $f \circ g$.
\item A slight variant of this method (which is essentially the same)
  is the substitution method, where we identify $g$ just as before,
  try to spot $g'$ in the integrand as before, and then put $u = g(x)$
  and rewrite the integral in terms of $u$.
\end{enumerate}

Actions ...

\begin{enumerate}
\item Good targets for $u$ in the $u$-substitution are things that are
  shielded.
\item In general, things in denominators, with non-integer exponents,
  or with complicated composites appended to them are good targets for
  $u$.
\end{enumerate}

\subsection{$u$-substitutions for definite integrals}

Words ... (try to recall the numerical formulations)

\begin{enumerate}
\item When doing the $u$-substitution for definite integrals, we
  transform the upper and lower limits of integration by the
  $u$-function.
\item Note that the $u$-substitution is valid only when the
  $u$-function is well-defined on the entire interval of integration.
\item The integral of a translate of a function is the integral of a
  function with the interval of integration suitably translated.
\item The integral of a multiplicative transform of a function is the
  integral of the function with the interval of integration
  transformed by the same multiplicative factor, scaled by that
  multiplicative factor.
\end{enumerate}

\subsection{Symmetry and integration}

Words ...

\begin{enumerate}
\item If a function is continuous and even, its integral on $[-a,0]$
  equals its integral on $[0,a]$. More generally, its integrals on any
  two segments that are reflections of each other about the origin are
  equal. As a corollary, the integral on $[-a,a]$ is twice the
  integral on $[0,a]$.
\item If a function is continuous and odd, its integral on $[-a,0]$ is
  the negative of its integral on $[0,a]$. More generally, its
  integrals on any two segments that are reflections of each other
  about the origin are negatives of each other. As a corollary, the
  integral on $[-a,a]$ is zero.
\item If a function is continuous and has mirror symmetry about the
  line $x = c$, its integral on $[c-h,c]$ equals its integral on
  $[c,c+h]$.
\item If a function is continuous and has half-turn symmetry about
  $(c,f(c))$, its integral on any interval of the form $[c-h,c+h]$ is
  $2hf(c)$. Basically, all the variation about $f(c)$ {\em cancels
  out} and the {\em average value} is $f(c)$.
\item Suppose $f$ is continuous and periodic with period $h$ and $F$
  is an antiderivative of $f$. The integral of $f$ over any interval
  of length $h$ is constant. Thus, $F(x + h) - F(x)$ is the same
  constant for all $x$. (We saw this fact long ago, without proof).
\item The constant mentioned above is zero iff $F$ is periodic, i.e.,
  $f$ has a periodic antiderivative.
\item There is thus a well-defined {\em average value} of a continuous
  periodic function on a period. This is also the average value of the
  same periodic function on any interval whose length is a nonzero
  integer multiple of the period. This is also the limit of the
  average value over very large intervals.
\end{enumerate}

Actions...

\begin{enumerate}
\item All this even-odd-periodic stuff is useful for trivializing some
  integral calculations without computing antiderivatives. This is
  more than an idle observation, since in a lot of real-world
  situations, we get functions that have some obvious symmetry, even
  though we know very little about the concrete form of the
  functions. We use this obvious symmetry to compute the integral.
\item Even if the whole integrand does not succumb to the lure of
  symmetry, it may be that the integrand can be written as (something
  nice and symmetric) + (something computable). The (nice and
  symmetric) part is then tackled using the ideas of symmetry, and the
  computable part is computed.
\end{enumerate}

\subsection{Mean-value theorem}

Words ...

\begin{enumerate}
\item The {\em average value}, or {\em mean value}, of a continuous
  function on an interval is the quotient of the integral of the
  function on the interval by the length of the interval.
\item The mean value theorem for integrals says that a continuous
  function must attain its mean value somewhere on the interior of the
  interval.
\item For periodic functions, the mean value over any interval whose
  length is a multiple of the period is the same. Also, the mean value
  over a very large interval approaches this value.
\item The mean value of a periodic continuous function $f$ being $0$
  means that $f$ has a periodic antiderivative; in fact, every
  antiderivative of $f$ is periodic. If the mean value is nonzero,
  every antiderivative of $f$ is periodic with shift, and the linear
  part of any antiderivative has slope equal to the mean value of $f$.
\end{enumerate}

Actions ...

\begin{enumerate}
\item The mean values (over a period) of $\sin$ and $\cos$ are
  $0$. For $\sin$, we can see this since it is an {\em odd} periodic
  function. $\cos$ is a translate of $\sin$, hence has the same mean
  value.
\item The mean values (over a period) of $\sin^2$ and $\cos^2$ are
  $1/2$. The functions add up to $1$, and are translates of each
  other, so this makes sense.
\item The mean value (over a period) of $x \mapsto f(mx)$, $m \ne 0$,
  is the same as the mean value of $f$, where $f$ is a continuous
  periodic function.
\item The mean value (over a period) of $|\sin|$ is $2/\pi$, and that
  of $\sin^+$ is $1/\pi$.
\end{enumerate}

\subsection{Application to area computations}

Words ...

\begin{enumerate}

\item We can use integration to determine the area of the region
  between the graph of a function $f$ and the $x$-axis from $x = a$ to
  $x = b$: this integral is $\int_a^b f(x) \, dx$. The integral
  measures the signed area: parts where $f \ge 0$ make positive
  contributions and parts where $f \le 0$ make negative
  contributions. The magnitude-only area is given as $\int_a^b |f(x)|
  \, dx$. The best way of calculating this is to split $[a,b]$ into
  sub-intervals such that $f$ has constant sign on each sub-interval,
  and add up the areas on each sub-interval.
\item Given two functions $f$ and $g$, we can measure the area between
  $f$ and $g$ between $x = a$ and $x = b$ as $\int_a^b |f(x) - g(x)|
  \,dx$. For practical purposes, we divide into sub-intervals so that
  on each sub-interval one function is bigger than the other. We then
  use integration to find the magnitude of the area on each
  sub-interval and add up. If $f$ and $g$ are both continuous, the
  points where the functions {\em cross} each other are points where
  $f = g$.
\end{enumerate}

Actions ...

\begin{enumerate}
\item In some situations we are directly given functions and/or curves
  and are asked to find areas. In others, we are given real-world
  situations where we need to find areas of regions. Here, we have to
  find functions and set up the integration problem as an intermediate
  step.
\item In all these situations, it is important to draw the graphs in a
  reasonably correct way. This brings us to all the ideas that are
  contained in graph drawing. Remember, here we may be interested in
  simultaneously graphing more than one function. Thus, in addition to
  being careful about each function, we should also correctly estimate
  where one function is bigger than the other, and find (approximately
  or exactly) the intersection points. (Go over the notes on
  graph-drawing, and some additional notes on graphing that weren't
  completely covered in class).
\item In some situations, we are asked to find the area(s) of
  region(s) bounded by the graphs of one, two, three, or more
  functions. Here, we first need to sketch the figure. Then, we need
  to find the interval of integration, and if necessary, split this
  interval into sub-intervals, such that on each sub-interval, we know
  exactly what integral we need to do. For instance, consider the
  region between the graphs of $\sin$, $\cos$, and the
  $x$-axis. Basically, the idea is to find, for all the vertical
  slices, the upper and lower limits of the slice.
\end{enumerate}


\end{document}
