\documentclass[10pt]{amsart}

%Packages in use
\usepackage{fullpage, hyperref, vipul, enumerate}

%Title details
\title{Take-home class quiz solutions: due Wednesday November 20: Image and kernel: applications to calculus}
\author{Math 196, Section 57 (Vipul Naik)}
%List of new commands

\begin{document}
\maketitle

\section{Performance review}

25 people took this 7-question quiz. The score distribution was as follows:

\begin{itemize}
\item Score of 1: 1 person
\item Score of 2: 3 people
\item Score of 3: 5 people
\item Score of 4: 3 people
\item Score of 5: 9 people
\item Score of 6: 3 people
\item Score of 7: 1 person
\end{itemize}

The question-wise answers and performance review were as follows:

\begin{enumerate}
\item Option (B): 4 people %$3$ people
\item Option (B): 8 people %$12$ people
\item Option (C): 18 people %$5$ people
\item Option (B): 19 people %$11$ people
\item Option (C): 14 people %$5$ people
\item Option (A): 18 people %$5$ people
\item Option (A): 23 people
\end{enumerate}

\section{Solutions}

{\bf PLEASE FEEL FREE TO DISCUSS {\em ALL} QUESTIONS.}

The goal of this quiz is to use the setting of calculus to practice
our skill of understanding linear transformations, specifically their
injectivity, surjectivity, bijectivity, kernel and image. It builds on
the November 8 quiz, but goes further. Please refer back to the
November 8 quiz for the definitions of vector space, subspace, and
linear transformation.

Please read these questions {\em very} carefully. For the first few
questions, the interpretation of the question in the language of
calculus is provided. Please refer to that if the linear algebra-based
description is unclear.

\begin{enumerate}

\item Let $\R[x]$ denote the vector space of all polynomials in one
  variable with real coefficients, with the usual addition and scalar
  multiplication of polynomials. There is an obvious linear
  transformation from $\R[x]$ to $C^\infty(\R)$ that sends any
  polynomial to the function it describes, e.g., the polynomial $x^2 +
  1$ gets sent to the function $x \mapsto x^2 + 1$. What can you say
  about this map $\R[x] \to C^\infty(\R)$?

  {\em Please note}: We are {\em not} talking here about whether the
  polynomial functions themselves are injective or surjective as
  functions from $\R$ to $\R$. Rather, we are talking about whether
  the mapping from {\em the set of polynomials} (which itself is a
  vector space over the reals) to {\em the set of infinitely
    differentiable functions} (which itself is another vector space).

  \begin{enumerate}[(A)]
  \item The map is neither injective nor surjective, i.e., different
    polynomials may define the same function, and not every infinitely
    differentiable function can be expressed using a polynomial.
  \item The map is injective but not surjective, i.e., different
    polynomials always define different functions, and not every
    infinitely differentiable function can be expressed using a
    polynomial.
  \item The map is surjective but not injective, i.e., different
    polynomials may define the same function, and every infinitely
    differentiable function can be expressed using a polynomial.
  \item The map is bijective, i.e., different polynomials always
    define different functions, and every infinitely differentiable
    function can be expressed using a polynomial.
  \end{enumerate}

  {\em Answer}: Option (B)

  {\em Explanation}: The map is linear, so to prove injectivity, it
  suffices to show that the kernel is zero. In other words, it
  suffices to show that if $p(x) \in \R[x]$ is in the kernel, then $p$
  is the zero polynomial.

  Suppose $p$ is in the kernel. Then, this means that $p(x) = 0$ for
  all $x \in \R$. This means that {\em every} real number is a root of
  $p$. But a nonzero polynomial can have only finitely many roots (the
  number of roots is at most equal to the degree of the polynomial), so
  this forces $p$ to be the zero polynomial.

  The map is not surjective because there exist lots of infinitely
  differentiable functions that are not polynomials. Examples include
  exponential and trigonometric functions.

  {\em Performance review}: 4 out of 25 got this. 12 chose (C), 7
  chose (D), 2 chose (A).

  {\em Historical note (last time)}: $3$ out of $26$ got
  this. $10$ chose (D), $8$ chose (C), and $5$ chose (A).

\item Denote by $\R[[x]]$ the vector space of all {\em formal power
  series} in one variable with real coefficients, with
  coefficient-wise addition and scalar multiplication. Explicitly, an
  element $a \in \R[[x]]$ is of the form:

  $$a = \sum_{i=0}^\infty a_ix^i = a_0 + a_1x + a_2x^2 + \dots$$

  where $a_i \in \R$ for $i \in \mathbb{N}_0$. Addition is
  coefficient-wise, i.e., if:

  $$a = \sum_{i=0}^\infty a_ix^i, b = \sum_{i=0}^\infty b_ix^i$$

  Then we have:

  $$a + b = \sum_{i=0}^\infty (a_i + b_i)x^i$$

  and for any real number $\lambda$, we have:

  $$\lambda a = \sum_{i=0}^\infty (\lambda a_i)x^i$$

  Note that a formal power series may have any radius of
  convergence. The radius of convergence could range from being $0$
  (which means that the formal power series converges only at the
  point $\{ 0 \}$) to being $\infty$ (which means that the formal
  power series converges on all of $\R$). In other words, a formal
  power series need not define an actual function on $\R$.

  {\em Aside}: If you remember sequences and series from
  single-variable calculus, you will recall that the radius of
  convergence is the reciprocal of the exponential growth rate of
  coefficients. In particular, if the coefficients {\em grow
    superexponentially}, the radius of convergence is zero. On the
  other hand, if the coefficients {\em decay superexponentially}, the
  radius of convergence is $\infty$. If the coefficients have
  exponential growth, the radius of convergence is less than $1$. If
  the coefficients have exponential decay, the radius of convergence
  is greater than $1$. Finally, if the coefficients grow or decay
  subexponentially, the radius of convergence is $1$.

  Note that $\R[x]$ can be viewed as a subspace of $\R[[x]]$ by
  thinking of each polynomial as a formal power series where there are
  only finitely many nonzero coefficients.

  Let $\Omega$ be the subset of $\R[[x]]$ comprising those formal power
  series that converge globally, i.e., the radius of convergence is
  $\infty$. Note that $\Omega$ is a sub{\em space} of $\R[[x]]$.

  What is the relation between $\R[x]$ and $\Omega$?

  Note that by {\em proper} subspace we mean a subspace that is not
  equal to the whole space.
  \begin{enumerate}[(A)]
  \item $\R[x] = \Omega$, i.e., a power series is globally convergent
    if and only if it is a polynomial (i.e., it has only finitely many
    nonzero coefficients).
  \item $\R[x]$ is a proper subspace of $\Omega$, i.e., every
    polynomial is a globally convergent power series, but there exist
    globally convergent power series that are not polynomials.
  \item $\Omega$ is a proper subspace of $\R[x]$, i.e., every globally
    convergent power series is a polynomial, but there are polynomials
    that are not globally convergent power series.
  \item $\R[x]$ and $\Omega$ are incomparable, i.e., there exist
    polynomials that are not globally convergent power series and
    there exist globally convergent power series that are not
    polynomials.
  \end{enumerate}

  {\em Answer}: Option (B)

  {\em Explanation}: The power series given by a polynomial converges,
  because it is a finite sum.

  However, there do exist globally convergent power series that are
  not polynomials. Specifically, any power series where the
  coefficients decay super-exponentially. An example is the power
  series of the exponential function.

  {\em Performance review}: 8 out of 25 got this. 8 chose (C), 5 chose
  (D), 4 chose (A).

  {\em Historical note (last time)}: $12$ out of $26$ got this. $11$ chose (C),
  $2$ chose (A), $1$ chose (D).

\item The {\em Taylor series operator} can be viewed as a linear
  transformation from $C^\infty(\R)$ to $\R[[x]]$. This operator sends
  any infinitely differentiable function to its Taylor series centered
  at $0$. Explicitly, the operator is:

  $$f \mapsto \sum_{k=0}^\infty \frac{f^{(k)}(0)}{k!}x^k$$

  What can we say about the kernel of this linear transformation?

  \begin{enumerate}[(A)]
  \item The kernel is the set of functions $f$ satisfying $f(0) = 0$
  \item The kernel is the set of functions $f$ satisfying $f'(0) = 0$
  \item The kernel is the set of functions $f$ such that $f$ and all
    its derivatives take the value $0$ at $0$.
  \item The kernel is the set of polynomial functions.
  \item The kernel is the set of functions that have globally
    convergent power series.
  \end{enumerate}

  {\em Answer}: Option (C)

  {\em Explanation}: This is direct from the definition.

  {\em Performance review}: 18 out of 25 got this. 4 chose (B), 2
  chose (E), 1 chose (A).

  {\em Historical note (last time)}: $5$ out of $26$ got this. $10$ chose (B),
  $9$ chose (A), $1$ each chose (D) and (E).

\item Which of the following is the best explanation for why we put
  the $+C$ when performing indefinite integration?

  \begin{enumerate}[(A)]
  \item The kernel of differentiation is a zero-dimensional space
    (namely, the zero function only), hence the fibers (inverse images
    or pre-images) for differentiation are all zero-dimensional spaces,
    i.e., single functions.
  \item The kernel of differentiation is a one-dimensional space
    (namely, the vector space of constant functions), hence the fibers
    (inverse images or pre-images) for differentiation are all
    one-dimensional spaces, i.e., lines that are translates of the
    space of constant functions.
  \item The image of differentiation is a zero-dimensional space
    (namely, the zero function only), hence the fibers (inverse images
    or pre-images) for differentiation are all zero-dimensional spaces,
    i.e., single functions.
  \item The image of differentiation is a one-dimensional space
    (namely, the vector space of constant functions), hence the fibers
    (inverse images or pre-images) for differentiation are all
    one-dimensional spaces, i.e., lines that are translates of the
    space of constant functions.
  \end{enumerate}

  {\em Answer}: Option (B)

  {\em Explanation}: This is obvious once you think about it.

  {\em Performance review}: 19 out of 25 got this. 4 chose (D), 2 chose (A).

  {\em Historical note (last time)}: $11$ out of $26$ got this. $7$ chose (C),
  $4$ chose (D), $3$ chose (A), $1$ chose (E).

\item When finding all functions $f$ on $\R$ such that $f''(x) = g(x)$
  for some known continuous function $g$ on $\R$, we get a general
  description of the form $G(x) + C_1x + C_2$ where $C_1$, $C_2$, are
  arbitrary real numbers. Which of the following is the best
  explanation for this?

  \begin{enumerate}[(A)]
  \item The kernel of the operation of differentiating twice is
    precisely the set of constant functions.
  \item The kernel of the operation of differentiating twice is
    precisely the set of nonconstant linear functions.
  \item The kernel of the operation of differentiating twice is the
    union of the set of constant functions and the set of nonconstant
    linear functions.
  \item The image of the operation of differentiating twice is
    precisely the set of constant functions.
  \item The image of the operation of differentiating twice is
    precisely the set of nonconstant linear functions.
  \end{enumerate}

  {\em Answer}: Option (C)

  {\em Explanation}: Note that the kernel is the set of functions of
  the form $x \mapsto C_1x + C_2$ where $C_1,C_2 \in \R$. Note that
  $C_1$ and $C_2$ are allowed to be equal to zero. In the case that
  $C_1 = 0$, we get constant functions, and in the case $C_1 \ne 0$,
  we get nonconstant linear functions. The kernel includes both types.

  {\em Performance review}: 14 out of 25 got this. 6 chose (A), 2 each
  chose (B) and (D), 1 chose (E).

  {\em Historical note (last time)}: $5$ out of $26$ got this. $11$ chose (A),
  $6$ chose (D), $3$ chose (B), $1$ chose (E).

\item Consider a second-order homogeneous linear differential equation
  of the form:

  $$y'' + p_1(x)y' + p_2(x)y = 0$$

  where $x$ is the independent variable, $y$ is the dependent
  variable, and $p_1$ and $p_2$ are known functions. We are trying to
  find global solutions, i.e., functions defined on all of $\R$. One
  way of thinking of this is to consider the linear transformation $L$
  that sends a function $y$ of $x$ to $L(y) = y'' + p_1(x)y' +
  p_2(x)y$, a new function of $x$. Which of the following best
  describes what we are trying to do?

  \begin{enumerate}[(A)]
  \item $L$ is a linear transformation $C^2(\R) \to C(\R)$, and the
    solution space we are interested in is the kernel of $L$.
  \item $L$ is a linear transformation $C^2(\R) \to C(\R)$, and the
    solution space we are interested in is the image of $L$.
  \item $L$ is a linear transformation $C(\R) \to C^2(\R)$, and the
    solution space we are interested in is the kernel of $L$.
  \item $L$ is a linear transformation $C(\R) \to C^2(\R)$, and the
    solution space we are interested in is the image of $L$.
  \end{enumerate}

  {\em Answer}: Option (A)

  {\em Explanation}: $L$ makes sense for functions in $C^2(\R)$ in so
  far as it requires differentiating twice. It does not make sense for
  other functions. The image of $L$ could (a priori) land anywhere in
  $C(\R)$. We are interested in the kernel of $L$, i.e., the functions
  whose image is $0$, because the left side of the differential
  equation is precisely computing $L(y)$.

  {\em Performance review}: 18 out of 25 got this. 4 chose (B), 2
  chose (C), 1 chose (D).

  {\em Historical note (last time)}: $5$ out of $26$ got this. $9$
  chose (B), $8$ chose (C), $4$ chose (D).

\item Consider a second-order non-homogeneous linear differential
  equation of the form:

  $$y'' + p_1(x)y' + p_2(x)y = q(x)$$

  where $x$ is the independent variable, $y$ is the dependent
  variable, and $p_1$, $p_2$, and $q$ are known functions. We are
  trying to find global solutions, i.e., functions defined on all of
  $\R$. One way of thinking of this is to consider the linear
  transformation $L$ that sends a function $y$ of $x$ to $L(y) = y'' +
  p_1(x)y' + p_2(x)y$, a new function of $x$. Which of the following
  best describes what we are trying to do?

  \begin{enumerate}[(A)]
  \item We are trying to find the inverse image under $L$ of $q(x)$,
    and we know this is a translate of the solution space of the
    corresponding homogeneous linear differential equation (the one
    from the preceding question).
  \item We are trying to find the image under $L$ of $p_1(x)$,
    and we know this is a translate of the solution space of the
    corresponding homogeneous linear differential equation (the one
    from the preceding question).
  \end{enumerate}

  {\em Answer}: Option (A)

  {\em Explanation}: This follows from the fibers being translates of
  the kernel.

  {\em Performance review}: 23 out of 25 got this. 2 chose (B).
\end{enumerate}
\end{document}
