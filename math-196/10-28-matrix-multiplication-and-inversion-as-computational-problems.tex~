\documentclass[10pt]{amsart}

%Packages in use
\usepackage{fullpage, hyperref, vipul, enumerate}

%Title details
\title{Take-home class quiz: due Monday October 28: Matrix multiplication and inversion as computational problems}
\author{Math 196, Section 57 (Vipul Naik)}
%List of new commands

\begin{document}
\maketitle

Your name (print clearly in capital letters): $\underline{\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad}$

This quiz tests for a strong {\em conceptualization} (i.e., a
metacognition) of the processes used for matrix multiplication and
inversion. It is based on part of the {\tt Matrix multiplication and
  inversion} notes and is related to Sections 2.3 and 2.4. It does
not, however, test all aspects of that material.

{\bf PLEASE FEEL FREE TO DISCUSS {\em ALL} QUESTIONS.}

\begin{enumerate}
\item How many arithmetic operations are needed for naive matrix
  multiplication of a $m \times n$ matrix and a $n \times p$ matrix?

  \begin{enumerate}[(A)]
  \item $O(mnp)$ additions and $O(mnp)$ multiplications
  \item $O(m + n + p)$ additions and $O(mnp)$ multiplications
  \item $O(mn)$ additions and $O(np)$ multiplications
  \item $O(mn + mp)$ additions and $O(mnp)$ multiplications
  \item $O(m + n + p)$ additions and $O(m + n + p)$ multiplications
  \end{enumerate}

  \vspace{0.1in}
  Your answer: $\underline{\qquad\qquad\qquad\qquad\qquad\qquad\qquad}$
  \vspace{0.1in}

\item What is the arithmetic complexity (in terms of total number of
  arithmetic operations needed) for naive matrix multiplication of two
  generic $n \times n$ matrices?

  \begin{enumerate}[(A)]
  \item $\Theta(n)$
  \item $\Theta(n^2)$
  \item $\Theta(n^3)$
  \item $\Theta(n^4)$
  \item $\Theta(n^5)$
  \end{enumerate}

  \vspace{0.1in}
  Your answer: $\underline{\qquad\qquad\qquad\qquad\qquad\qquad\qquad}$
  \vspace{0.1in}

\item Which of the following is the tightest ``obvious'' lower bound
  on the possible arithmetic complexity of any generic algorithm for
  multiplying two $n \times n$ matrices? We use $\Omega$ to denote
  {\em at least that order}.

  \begin{enumerate}[(A)]
  \item $\Omega(n)$
  \item $\Omega(n^2)$
  \item $\Omega(n^3)$
  \item $\Omega(n^4)$
  \item $\Omega(n^5)$
  \end{enumerate}

  \vspace{0.1in}
  Your answer: $\underline{\qquad\qquad\qquad\qquad\qquad\qquad\qquad}$
  \vspace{0.1in}

\item What is the minimum number of arithmetic operations needed to
  compute the product of two generic diagonal $n \times n$ matrices?

  \begin{enumerate}[(A)]
  \item $n$
  \item $n + 1$
  \item $2n - 1$
  \item $2n$
  \item $n^2$
  \end{enumerate}

  \vspace{0.1in}
  Your answer: $\underline{\qquad\qquad\qquad\qquad\qquad\qquad\qquad}$
  \vspace{0.1in}

\item What is the minimum number of arithmetic operations needed to
  compute the product of a generic $1 \times n$ matrix and a generic
  $n \times 1$ matrix?

  \begin{enumerate}[(A)]
  \item $n$
  \item $n + 1$
  \item $2n - 1$
  \item $2n$
  \item $n^2$
  \end{enumerate}

  \vspace{0.1in}
  Your answer: $\underline{\qquad\qquad\qquad\qquad\qquad\qquad\qquad}$
  \vspace{0.1in}

\item What is the minimum number of arithmetic operations needed to
  compute the product of a generic $n \times 1$ matrix and a generic
  $1 \times n$ matrix?

  \begin{enumerate}[(A)]
  \item $n$
  \item $n + 1$
  \item $2n - 1$
  \item $2n$
  \item $n^2$
  \end{enumerate}

  \vspace{0.1in}
  Your answer: $\underline{\qquad\qquad\qquad\qquad\qquad\qquad\qquad}$
  \vspace{0.1in}


\item In order terms, what is the minimum number of arithmetic
  operations needed to compute the product of a generic $n \times n$
  diagonal matrix and a generic $n \times n$ upper triangular matrix?
  The upper triangular matrix has zero entries below the diagonal. The
  entries on or above the diagonal may be nonzero (and generically,
  they will be nonzero).

  \begin{enumerate}[(A)]
  \item $n(n - 1)/2$
  \item $n(n + 1)/2$
  \item $n(n - 1)$
  \item $n^2$
  \item $n(n + 1)$
  \end{enumerate}

  \vspace{0.1in}
  Your answer: $\underline{\qquad\qquad\qquad\qquad\qquad\qquad\qquad}$
  \vspace{0.1in}

  Adding $n$ numbers to each other requires $n - 1$ addition
  operations. In a non-parallel setting, there is no way of improving
  this.

  However, using the associativity of addition, we can write a faster
  parallelizable algorithm. A simple parallelization is to split the
  list being added into two sublists of length about $n/2$
  each. Delegate the task of adding up within each sublist to
  different processors running in parallel. Then, add up the numbers
  obtained. This takes about half the time, with a little overhead (of
  collecting and adding up). This type of strategy is called a {\em
    divide and conquer} strategy. Using a divide and conquer strategy
  repeatedly, we can demonstrate that the parallelized arithmetic
  complexity of this approach is $\Theta(\log_2n)$.

\item Suppose $A$ is a $1 \times n$ matrix and $B$ is a $n \times 1$
  matrix. Assume an unlimited number of processors that all have free
  read access to both $A$ and $B$, free write access to the product
  matrix, and a shared workspace where they can store intermediate
  results. What is the arithmetic complexity in this context (i.e.,
  the parallelized arithmetic complexity) for computing $AB$? What we
  mean here is: what is the smallest depth of a computational tree to
  compute $AB$?

  \begin{enumerate}[(A)]
  \item $\Theta(1)$
  \item $\Theta(\log_2 n)$
  \item $\Theta(n \log_2 n)$
  \item $\Theta(n^2)$
  \item $\Theta(n^2 \log_2 n)$
  \end{enumerate}

  \vspace{0.1in}
  Your answer: $\underline{\qquad\qquad\qquad\qquad\qquad\qquad\qquad}$
  \vspace{0.1in}

\item Suppose $A$ is a $n \times 1$ matrix and $B$ is a $1 \times n$
  matrix. Assume an unlimited number of processors that all have free
  read access to both $A$ and $B$, free write access to the product
  matrix, and a shared workspace where they can store intermediate
  results. What is the arithmetic complexity in this context (i.e.,
  the parallelized arithmetic complexity) for computing $AB$? What we
  mean here is: what is the smallest depth of a computational tree to
  compute $AB$?

  \begin{enumerate}[(A)]
  \item $\Theta(1)$
  \item $\Theta(\log_2 n)$
  \item $\Theta(n \log_2 n)$
  \item $\Theta(n^2)$
  \item $\Theta(n^2 \log_2 n)$
  \end{enumerate}

  \vspace{0.1in}
  Your answer: $\underline{\qquad\qquad\qquad\qquad\qquad\qquad\qquad}$
  \vspace{0.1in}

\item Suppose $A$ and $B$ are two $n \times n$ matrices. Assume an
  unlimited number of processors that all have free read access to
  both $A$ and $B$, free write access to the product matrix, and a
  shared workspace where they can store intermediate results. What is
  the arithmetic complexity in this context (i.e., the parallelized
  arithmetic complexity) for computing $AB$? What we mean here is:
  what is the smallest depth of a computational tree to compute $AB$?
  Use naive matrix multiplication and speed it up using the
  parallelized processes discused here.

  \begin{enumerate}[(A)]
  \item $\Theta(1)$
  \item $\Theta(\log_2 n)$
  \item $\Theta(n \log_2 n)$
  \item $\Theta(n^2)$
  \item $\Theta(n^2 \log_2 n)$
  \end{enumerate}

  \vspace{0.1in}
  Your answer: $\underline{\qquad\qquad\qquad\qquad\qquad\qquad\qquad}$
  \vspace{0.1in}
  
  We are given a $n \times n$ matrix $A$ and we want to use {\em
    repeated squaring} to calculate powers of $A$. For instance, to
  calculate $A^4$, we can simply calculate $(A^2)^2$, which requires
  two multiplications. To calculate $A^5$, we calculate $(A^2)^2A$,
  which requires three multiplications. Assume that we can store any
  number of intermediate matrices, i.e., storage space is not a
  constraint.

\item What is the smallest number of matrix multiplications needed to
  calculate $A^7$ using repeated squaring?

  \begin{enumerate}[(A)]
  \item $3$
  \item $4$
  \item $5$
  \item $6$
  \item $7$
  \end{enumerate}

  \vspace{0.1in}
  Your answer: $\underline{\qquad\qquad\qquad\qquad\qquad\qquad\qquad}$
  \vspace{0.1in}

\item What is the smallest number of matrix multiplications needed to
  calculate $A^8$ using repeated squaring?

  \begin{enumerate}[(A)]
  \item $3$
  \item $4$
  \item $5$
  \item $6$
  \item $7$
  \end{enumerate}

  \vspace{0.1in}
  Your answer: $\underline{\qquad\qquad\qquad\qquad\qquad\qquad\qquad}$
  \vspace{0.1in}

\item What is the smallest number of matrix multiplications needed to
  calculate $A^{21}$ using repeated squaring?

  \begin{enumerate}[(A)]
  \item $3$
  \item $4$
  \item $5$
  \item $6$
  \item $7$
  \end{enumerate}

  \vspace{0.1in}
  Your answer: $\underline{\qquad\qquad\qquad\qquad\qquad\qquad\qquad}$
  \vspace{0.1in}

  Suppose $A$ is an {\em invertible} $n \times n$ matrix. It is
  possible to invert $A$ using $\Theta(n^3)$ (worst-case) arithmetic
  operations via Gauss-Jordan elimination. We can thus add computation
  of the inverse to our toolkit when calculating powers. It is helpful
  even when calculating positive powers.

  Count each matrix multiplication and each matrix inversion as one
  ``matrix operation.''

\item What is the smallest positive $r$ where we can achieve a saving
  on the total number of matrix operations to calculate $A^r$ by also
  computing $A^{-1}$, rather than just using repeated squaring?

  \begin{enumerate}[(A)]
  \item $3$
  \item $7$
  \item $15$
  \item $23$
  \item $31$
  \end{enumerate}

  \vspace{0.1in}
  Your answer: $\underline{\qquad\qquad\qquad\qquad\qquad\qquad\qquad}$
  \vspace{0.1in}

\item {\em Strassen's algorithm} is a {\em fast matrix multiplication}
  algorithm that can multiply two $n \times n$ matrices using
  $O(n^{\log_27})$ arithmetic operations. In practice, however, a lot
  of existing computer code for matrix multiplication, written long
  after Strassen's algorithm was discovered, uses naive matrix
  multiplication. Which of the following reasons explain this? Please
  see Options (D) and (E) before answering.

  \begin{enumerate}[(A)]
  \item Strassen's algorithm becomes faster than naive matrix
    multiplication only for very large matrix sizes.
  \item Strassen's algorithm is more complicated to code.
  \item Strassen's algorithm is not as easily parallelizable as naive
    matrix multiplication.
  \item All of the above.
  \item None of the above.
  \end{enumerate}

  \vspace{0.1in}
  Your answer: $\underline{\qquad\qquad\qquad\qquad\qquad\qquad\qquad}$
  \vspace{0.1in}

  There exist even faster algorithms for matrix multiplication than
  Strassen's algorithm. The best known algorithm currently is the {\em
    Coppersmith-Winograd algorithm}, which can multiply two $n \times
  n$ matrices in time $O(n^{2.3727})$. However, the
  Coppersmith-Winograd algorithm is even more rarely implemented than
  Strassen's for practical matrix multiplication code (according to
  some sources, Coppersmith-Winograd has {\em never} been
  implemented). The same reasons as those cited above for the
  reluctance to use Strassen's algorithm apply. There are some
  additional obstacles to practical implementations of the
  Coppersmith-Winograd algorithm that make it even more difficult to
  use.

\item Suppose $A$ and $B$ are $n \times n$ matrices. What is the
  minimum number of matrix multiplications needed generically to
  compute the product $ABABABABA$?

   \begin{enumerate}[(A)]
   \item $4$
   \item $5$
   \item $6$
   \item $7$
   \item $8$
   \end{enumerate}

   \vspace{0.1in}
   Your answer: $\underline{\qquad\qquad\qquad\qquad\qquad\qquad\qquad}$
   \vspace{0.1in}


\end{enumerate}
\end{document}
