\documentclass[10pt]{amsart}

%Packages in use
\usepackage{fullpage, hyperref, vipul, enumerate}

%Title details
\title{Take-home class quiz: due Monday November 25: Subspace, basis, dimension, and abstract spaces: applications to calculus}
\author{Math 196, Section 57 (Vipul Naik)}
%List of new commands

\begin{document}
\maketitle

Your name (print clearly in capital letters): $\underline{\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad}$

{\bf PLEASE FEEL FREE TO DISCUSS {\em ALL} QUESTIONS.}

This quiz builds on the November 8 and November 20 quizzes that apply
ideas we are learning about linear transformations to the calculus
setting. The November 8 quiz went over some basic ideas related to
differentiation as a linear transformation. The November 20 quiz
explored the ideas in greater depth. We now look at questions that
apply the ideas of basis, dimension, and subspace to the calculus
setting.

We begin by recalling some notation and facts we already saw in
earlier quizzes.  Denote by $C(\R)$ (or alternatively by $C^0(\R)$)
the vector space of all continuous functions from $\R$ to $\R$, with
pointwise addition and scalar multiplication. Note that the elements
of this vector space, which we would ordinarily call ``vectors'', are
now {\em functions}.

For $k$ a positive integer, denote by $C^k(\R)$ the subspace of
$C(\R)$ comprising those continuous functions that are at least $k$
times {\em continuously} differentiable. Note that $C^{k+1}(\R)$ is
a subspace of $C^k(\R)$, so we have a descending chain of subspaces:

$$C(\R) = C^0(\R) \supseteq C^1(\R) \supseteq C^2(\R) \supseteq \dots $$

The intersection of these spaces is the vector space $C^\infty(\R)$,
defined as the subspace of $C(\R)$ comprising those functions that
are {\em infinitely} differentiable.

We had also noted that:

\begin{itemize}
\item The kernel of differentiation is the vector space of constant functions.
\item The kernel of $k$ times differentiating is the vector space of
  polynomials of degree at most $k - 1$.
\item The fiber of any function for differentiation is a translate
  of the space of constant functions. That's what explains the $+C$
  when you perform indefinite integration.
\end{itemize}

{\em Note}: For finite-dimensional spaces, a linear transformation $T$
from a vector space to itself is injective if and only if it is
surjective. This follows from dimension and rank considerations: $T$
is injective if and only its kernel is zero, which happens if and only
if the matrix has full column rank, which happens if and only if the
matrix has full row rank (because the matrix is a square matrix),
which happens if and only if $T$ is surjective. The rank-nullity
theorem provides an equivalent explanation. We had also seen that if
$T:\R^m \to \R^n$ is injective, then $m \le n$, and if $T: \R^m \to
\R^n$ is surjective, then $m \ge n$. In particular, we cannot have a
surjective map from a proper subspace to the whole space.

With infinite-dimensional spaces, however, we can have funny
phenomena. Examples of these phenomena are strewn across the quizzes.

\begin{itemize}
\item We can have a map from an infinite-dimensional vector space to
  itself that is injective but not surjective.
\item We can have a map from an infinite-dimensional vector space to
  itself that is surjective but not injective.
\item We can have a surjective map from a proper subspace to the whole
  space (for instance, differentiation $C^1(\R) \to C(\R)$ is
  surjective, even though $C^1(\R)$ is a proper subspace of $C(\R)$).
\item We can have an injective map from a space to a proper subspace.
\end{itemize}

Note that we will use the terms {\em subspace} and {\em vector
  subspace} synonymously with {\em linear subspace} in this quiz.

\vspace{0.5in}
\begin{enumerate}

\item Suppose $V$ is a vector subspace of the vector space
  $C^\infty(\R)$. We know that differentiation is linear. How is that
  information computationally useful?

  \begin{enumerate}[(A)]
  \item It tells us that knowing how to differentiate all functions in
    any spanning set for $V$ tells us how to differentiate any
    function in $V$ (assuming we know how to express any function in
    $V$ as a linear combination of the functions in the spanning set).
  \item It tells us that knowing how to differentiate all functions in
    any linearly independent set in $V$ tells us how to differentiate any
    function in $V$.
  \end{enumerate}

  \vspace{0.1in}
  Your answer: $\underline{\qquad\qquad\qquad\qquad\qquad\qquad\qquad}$
  \vspace{0.1in}

\item Suppose $V$ is a vector subspace of the vector space
  $C^\infty(\R)$. We know that differentiation is linear. How
  is that information computationally useful?

  \begin{enumerate}[(A)]
  \item It tells us that knowing the antiderivatives of all functions
    in any spanning set for $V$ tells us the antiderivative of every
    function in $V$ (assuming we know how to express any function in
    $V$ as a linear combination of the functions in the spanning set).
  \item It tells us that knowing the antiderivatives of all functions in
    any linearly independent set in $V$ tells us the antiderivative of every
    function in $V$.
  \end{enumerate}

  \vspace{0.1in}
  Your answer: $\underline{\qquad\qquad\qquad\qquad\qquad\qquad\qquad}$
  \vspace{0.1in}

  We now consider two related vector spaces. $\R[x]$ is defined as the
  vector space of polynomials with real coefficients in the single
  variable $x$, with the usual addition and scalar
  multiplication. There is a natural injective homomorphism from
  $\R[x]$ to $C^\infty(\R)$ that sends any polynomial to the same
  polynomial viewed as a function.

  $\R(x)$ is defined as the vector space of all rational functions
  where the numerator and denominator are both polynomials with the
  denominator nonzero, up to equivalence (i.e., two rational functions
  $p_1(x)/q_1(x)$ and $p_2((x)/q_2(x)$ are equivalent if $p_1(x)q_2(x)
  = q_1(x)p_2(x)$). Addition and scalar multiplication are defined the
  usual way. Note that there is a natural injective homomorphism from
  $\R[x]$ to $\R(x)$ that sends any polynomial $p(x)$ to the rational
  function $p(x)/1$.

  Also note that $\R(x)$ does not map to $C^\infty(\R)$, for the
  reason that a rational function, viewed {\em qua} function, is not
  necessarily defined everywhere. Specifically, if written in
  simplified form, it is not defined at the set of roots of its
  denominator.

  Note that both $\R[x]$ and $\R(x)$ are infinite-dimensional vector
  spaces, i.e., they do not have finite spanning sets.

\item Which of the following is {\em not} a basis for $\R[x]$? Please
  see Option (E) before answering.

  \begin{enumerate}[(A)]
  \item $1,x,x^2,x^3,\dots$
  \item $1,x,x(x - 1),x(x - 1)(x - 2),x(x-1)(x-2)(x-3),\dots$
  \item $1,x + 1, x^2 + x + 1, x^3 + x^2 + x + 1, \dots$
  \item $1,x,x^2 - x, x^3 - x^2, x^4 - x^3,\dots$
  \item None of the above, i.e., each of them is a basis.
  \end{enumerate}

  \vspace{0.1in}
  Your answer: $\underline{\qquad\qquad\qquad\qquad\qquad\qquad\qquad}$
  \vspace{0.1in}

  Let's now revisit the topic of {\em partial fractions} as a tool for
  integrating rational functions. The idea behind partial fractions is
  to consider an integration problem with respect to a variable
  $x$ with integrand of the following form:

  $$\frac{a_0 + a_1x + a_2x^2 + \dots + a_{n-1}x^{n-1}}{p(x)}$$

  where $p$ is a polynomial of degree $n$. For convenience, we may
  take $p$ to be a monic polynomial, i.e., a polynomial with leading
  coefficient $1$. For $p$ fixed, the set of all rational functions of
  the form above forms a vector subspace of dimension $n$ inside
  $\R(x)$. A natural choice of basis for this subspace is:

  $$\frac{1}{p(x)}, \frac{x}{p(x)}, \dots, \frac{x^{n-1}}{p(x)}$$

  The goal of partial fraction theory is to provide an {\em alternate
    basis} for this space of functions with the property that those
  basis elements are particularly easy to integrate (recurring to one
  of our earlier questions). Let's illustrate one special case: the
  case that $p$ has $n$ distinct real roots
  $\alpha_1,\alpha_2,\dots,\alpha_n$. The alternate basis in this case is:

  $$\frac{1}{x - \alpha_1}, \frac{1}{x - \alpha_2}, \dots, \frac{1}{x - \alpha_n}$$

  The explicit goal is to rewrite a partial fraction:

  $$\frac{a_0 + a_1x + a_2x^2 + \dots + a_{n-1}x^{n-1}}{p(x)}$$

  in terms of the basis above. If we denote the numerator as $r(x)$, we want to write:

  $$\frac{r(x)}{p(x)} = \frac{c_1}{x - \alpha_1} + \frac{c_2}{x - \alpha_2} + \dots + \frac{c_n}{x - \alpha_n}$$

  The explicit formula is:

  $$c_i = \frac{r(\alpha_i)}{\prod_{j \ne i} (\alpha_i - \alpha_j)}$$

  Once we rewrite the original rational function as a linear
  combination of the new basis vectors, we can integrate it easily
  because we know the antiderivatives of each of the basis
  vectors. The antiderivative is thus:

  $$\left(\sum_{i=1}^n \frac{r(\alpha_i)}{\prod_{j \ne i} (\alpha_i - \alpha_j)} \ln|x - \alpha_i|\right) + C$$

  where the obligatory $+C$ is put for the usual reasons.

  Note that this process only handles rational functions that are
  proper fractions, i.e., the degree of the numerator must be less
  than that of the denominator.

  We now consider cases where $p$ is a polynomial of a different type.

\item Suppose $p$ is a monic polynomial of degree $n$ that is a
  product of pairwise distinct irreducible factors that are all either
  monic linear or monic quadratic. Call the roots for the linear
  polynomials $\alpha_1, \alpha_2,\dots,\alpha_s$ and call the monic
  quadratic factors $q_1,q_2,\dots,q_t$. Which of the following sets
  forms a basis for the vector space that we are interested in, namely
  all rational functions of the form $r(x)/p(x)$ where the degree of
  $r$ is less than $n$? Please see Option (E) before answering.

  \begin{enumerate}[(A)]
  \item All rational functions of the form $1/(x - \alpha_i), 1 \le i
    \le s$ together with all rational functions of the form $1/q_j(x),
    1 \le j \le t$
  \item All rational functions of the form $1/(x - \alpha_i), 1 \le i
    \le s$ together with all rational functions of the form $q_j'(x)/q_j(x),
    1 \le j \le t$
  \item All rational functions of the form $1/q_j(x), 1 \le j \le t$
    together with all rational functions of the form $q_j'(x)/q_j(x),
    1 \le j \le t$
  \item All rational functions of the form $1/(x - \alpha_i), 1 \le i
    \le s$ together with all rational functions of the form $1/q_j(x),
    1 \le j \le t$ {\em and} all rational functions of the form
    $q_j'(x)/q_j(x), 1 \le j \le t$
  \item None of the above
  \end{enumerate}

  \vspace{0.1in}
  Your answer: $\underline{\qquad\qquad\qquad\qquad\qquad\qquad\qquad}$
  \vspace{0.1in}

\item Suppose $p(x) = (x - \alpha)^n$. Which of the following sets
  forms a basis for the vector space that we are interested in, namely
  all rational functions of the form $r(x)/p(x)$ where the degree of
  $r$ is less than $n$? Please see Options (D) and (E) before
  answering.

  \begin{enumerate}[(A)]
  \item The single function $1/(x - \alpha)$
  \item The single function $1/(x - \alpha)^n$
  \item All the functions $1/(x - \alpha), 1/(x - \alpha)^2, \dots, 1/(x - \alpha)^n$
  \item Any of the above works
  \item None of the above works
  \end{enumerate}

  \vspace{0.1in}
  Your answer: $\underline{\qquad\qquad\qquad\qquad\qquad\qquad\qquad}$
  \vspace{0.1in}

  We now recall our earlier discussion of the solution process for
  first-order linear differential equations. Consider a first-order
  linear differential equation with independent variable $x$ and
  dependent variable $y$, with the equation having the form:

  $$y' + p(x)y = q(x)$$

  where $p,q \in C^\infty(\R)$.

  We solve this equation as follows. Let $H$ be an antiderivative of
  $p$, so that $H'(x) = p(x)$. 

  $$\frac{d}{dx}\left(ye^{H(x)}\right) = q(x)e^{H(x)}$$

  This gives:

  $$ye^{H(x)}  = \int q(x)e^{H(x)} \, dx$$
  
  So:

  $$y = e^{-H(x)}\int q(x)e^{H(x)} \, dx$$

  The indefinite integration gives a $+C$, so overall, we get:

  $$y = Ce^{-H(x)} + \text{particular solution}$$

  It's now time to understand this in terms of linear algebra.

  Define a linear transformation $L:C^\infty(\R) \to C^\infty(\R)$ as:

  $$f(x) \mapsto f'(x) + p(x)f(x)$$

\item The kernel of $L$ is one-dimensional. Which of the following
  functions spans the kernel?

  \begin{enumerate}[(A)]
  \item $p(x)$
  \item $q(x)$
  \item $H(x)$
  \item $e^{H(x)}$
  \item $e^{-H(x)}$
  \end{enumerate}

  \vspace{0.1in}
  Your answer: $\underline{\qquad\qquad\qquad\qquad\qquad\qquad\qquad}$
  \vspace{0.1in}

\item I would like to argue that $L$ is {\em surjective} as a linear
  transformation from $C^\infty(\R)$ to $C^\infty(\R)$. Why is that
  true?

  \begin{enumerate}[(A)]
  \item The kernel of $L$ is zero-dimensional.
  \item The image of $L$ is zero-dimensional.
  \item The kernel of $L$ is one-dimensional.
  \item The image of $L$ is one-dimensional.
  \item For any $q$, we have a formula above that describes a solution
    function that maps to $q$.
  \end{enumerate}

  \vspace{0.1in}
  Your answer: $\underline{\qquad\qquad\qquad\qquad\qquad\qquad\qquad}$
  \vspace{0.1in}

  Let $n$ be a nonnegative integer. Denote by $P_n$ the vector space
  of all polynomials in one variable $x$ that have degree $\le
  n$. $P_n$ is a subspace of $\R[x]$, which in turn can be viewed as a
  subspace of $C^\infty(\R)$ through the natural injective map. For
  convenience and completeness, define $P_{-1}$ to be the zero
  subspace.

  Differentiation defines a linear transformation from
  $C^\infty(\R)$ to itself.

\item What are the kernel and image of the restriction of
  differentiation to $P_n$? The result should be valid for all
  positive integers $n$.
 
  \begin{enumerate}[(A)]
  \item The kernel and image are both $P_n$
  \item The kernel is the zero subspace and the image is $P_n$
  \item The kernel is $P_n$ and the image is the zero subspace
  \item The kernel is $P_{n-1}$ and the image is $P_0$ (the subspace
    of constant functions)
  \item The kernel is $P_0$ and the image is $P_{n-1}$
  \end{enumerate}
  
  \vspace{0.1in}
  Your answer: $\underline{\qquad\qquad\qquad\qquad\qquad\qquad\qquad}$
  \vspace{0.1in}

\item What are the kernel and image of the restriction of
  differentiation to all of $\R[x]$?

  \begin{enumerate}[(A)]
  \item The kernel and image are both $\R[x]$
  \item The kernel is the zero subspace and the image is $\R[x]$
  \item The kernel is $\R[x]$ and the image is the zero subspace
  \item The kernel is $\R[x]$ and the image is $P_0$ (the subspace
    of constant functions)
  \item The kernel is $P_0$ and the image is $\R[x]$
  \end{enumerate}
  
  \vspace{0.1in}
  Your answer: $\underline{\qquad\qquad\qquad\qquad\qquad\qquad\qquad}$
  \vspace{0.1in}

\item We can use differentiation to define a linear transformation
  from $\R(x)$ to $\R(x)$, where we differentiate a rational function
  using the quotient rule for differentiation and the known rules for
  differentiating polynomials. What can we say about this linear
  transformation?

  \begin{enumerate}[(A)]
  \item The differentiation linear transformation is bijective from
    $\R(x)$ to $\R(x)$, i.e., every rational function is the
    derivative of a unique rational function.
  \item The differentiation linear transformation is injective but not
    surjective from $\R(x)$ to $\R(x)$, i.e., every rational function
    is the derivative of {\em at most one} rational function, but
    there do exist rational functions that are not expressible as the
    derivative of any rational function.
  \item The differentiation linear transformation is surjective but
    not injective from $\R(x)$ to $\R(x)$, i.e., every rational
    function is the derivative of {\em at least one} rational
    function, but there do exist rational functions that occur as
    derivatives of more than one rational function.
  \item The differentiation linear transformation is neither injective
    nor surjective from $\R(x)$ to $\R(x)$.
  \end{enumerate}

  \vspace{0.1in}
  Your answer: $\underline{\qquad\qquad\qquad\qquad\qquad\qquad\qquad}$
  \vspace{0.1in}

\item Denote by $\R[[x]]$ the vector space of all formal power series
  with real coefficients in one variable, i.e., series of the form:

  $$\sum_{i=0}^\infty a_ix^i$$

  Formal differentiation defines a linear transformation from
  $\R[[x]]$ to itself. What can we say about this linear transformation?

  \begin{enumerate}[(A)]
  \item The formal differentiation linear transformation is bijective
    from $\R[[x]]$ to $\R[[x]]$.
  \item The formal differentiation linear transformation is injective
    but not surjective from $\R[[x]]$ to $\R[[x]]$.
  \item The formal differentiation linear transformation is surjective but
    not injective from $\R[[x]]$ to $\R[[x]]$.
  \item The formal differentiation linear transformation is neither
    injective nor surjective from $\R[[x]]$ to $\R[[x]]$.
  \end{enumerate}

  \vspace{0.1in}
  Your answer: $\underline{\qquad\qquad\qquad\qquad\qquad\qquad\qquad}$
  \vspace{0.1in}

\item Consider the following two linear transformations $T_1,T_2:\R[x]
  \to \R[x]$: $T_1$ is differentiation, and $T_2$ is multiplication by
  $x$. Which of the following is true?

  \begin{enumerate}[(A)]
  \item Both $T_1$ and $T_2$ are injective, but neither is surjective.
  \item Both $T_1$ and $T_2$ are surjective, but neither is injective.
  \item $T_1$ is injective but not surjective. $T_2$ is surjective but not injective.
  \item $T_1$ is surjective but not injective. $T_2$ is injective but
    not surjective.
  \item Neither $T_1$ nor $T_2$ is injective. Neither $T_1$ nor $T_2$
    is surjective.
  \end{enumerate}

  \vspace{0.1in}
  Your answer: $\underline{\qquad\qquad\qquad\qquad\qquad\qquad\qquad}$
  \vspace{0.1in}

\item Consider the linear transformations $T_1$ and $T_2$ of the
  preceding question. What can we say regarding whether $T_1$ and
  $T_2$ commute?

  \begin{enumerate}[(A)]
  \item $T_1$ and $T_2$ commute.
  \item $T_1$ and $T_2$ do not commute.
  \end{enumerate}

  \vspace{0.1in}
  Your answer: $\underline{\qquad\qquad\qquad\qquad\qquad\qquad\qquad}$
  \vspace{0.1in}
\end{enumerate}
\end{document}
